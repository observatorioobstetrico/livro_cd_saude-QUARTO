# Aprendizado N√£o Supervisionado

O aprendizado de m√°quina pode ser dividido em duas categorias principais: aprendizado supervisionado e aprendizado n√£o supervisionado. O aprendizado supervisionado √© uma abordagem que utiliza um conjunto de dados rotulados para treinar um modelo de aprendizado de m√°quina. Por outro lado, o aprendizado n√£o supervisionado n√£o utiliza r√≥tulos, mas em vez disso, tenta encontrar padr√µes e estruturas no conjunto de dados sem qualquer orienta√ß√£o externa.

Em geral, o aprendizado n√£o supervisionado √© usado quando n√£o h√° classifica√ß√£o sobre os dados dispon√≠vel ou quando os dados rotulados s√£o muito escassos. Ele pode ser usado para v√°rias tarefas, incluindo clustering (agrupamento) e redu√ß√£o de dimensionalidade.


### Clustering ou Agrupamentos

O clustering √© uma t√©cnica de aprendizado n√£o supervisionado em que os dados s√£o agrupados com base em sua similaridade. Em outras palavras, os dados que s√£o mais semelhantes s√£o postos em uma mesma classifica√ß√£o, enquanto os dados que s√£o diferentes s√£o agrupados em classifica√ß√µes diferentes.

Existem v√°rios algoritmos de clustering dispon√≠veis, incluindo o algoritmo k-means, o clustering hier√°rquico e o DBSCAN. O algoritmo k-means √© um dos algoritmos de clustering mais populares por conta da sua f√°cil compreens√£o e aplica√ß√£o, al√©m de sua efici√™ncia.

### Redu√ß√£o de dimensionalidade

A redu√ß√£o de dimensionalidade √© uma t√©cnica de aprendizado n√£o supervisionado usada para reduzir o n√∫mero de vari√°veis em um conjunto de dados. Ou seja, reduz a dimensionalidade do espa√ßo de caracter√≠sticas dos dados, enquanto tenta preservar a maior parte da informa√ß√£o original. A redu√ß√£o de dimensionalidade pode ser usada para simplificar o modelo e, portanto, torn√°-lo mais f√°cil de interpretar ou para acelerar o treinamento do modelo.

Existem v√°rios algoritmos de redu√ß√£o de dimensionalidade dispon√≠veis, incluindo a An√°lise de Componentes Principais (PCA), a An√°lise Fatorial e a T-SNE. O PCA √© um dos algoritmos de redu√ß√£o de dimensionalidade mais populares e ser√° o aqui trabalhado.

## An√°lise de Agrupamentos

```{r include=FALSE}
dados_indicadores <- readRDS('dados/dados_indicadores.rds')                                       

```

Como descrito anteriormente e refor√ßado aqui, na an√°lise de agrupamento, buscamos identificar regi√µes no espa√ßo dos dados que possuam um grande n√∫mero de observa√ß√µes pr√≥ximas umas das outras. Essas regi√µes s√£o chamadas de clusters. A ideia √© agrupar indiv√≠duos que sejam semelhantes entre si e diferentes dos indiv√≠duos em outros clusters. Essa t√©cnica √© chamada de aprendizado n√£o supervisionado, pois n√£o utilizamos uma vari√°vel espec√≠fica como refer√™ncia para avaliar o resultado do agrupamento.

Formalmente, os clusters s√£o definidos da seguinte forma:

-   Cada cluster √© um grupo de observa√ß√µes;

-   Todos os indiv√≠duos pertencem a pelo menos um cluster;

-   Dois clusters diferentes n√£o possuem observa√ß√µes em comum.

Ao realizar o agrupamento de dados, √© importante utilizar um m√©todo que maximize as diferen√ßas entre os clusters, ao mesmo tempo que minimiza as diferen√ßas dentro de cada cluster. Para isso, s√£o utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferen√ßas entre as observa√ß√µes.

As medidas de dissimilaridade mais comumente usadas s√£o a dist√¢ncia euclidiana e a dist√¢ncia euclidiana quadr√°tica, como apresentado abaixo respectivamente:

$$
\begin{split}
d(\mathbf{x}_i, \mathbf{x}_i') = \sqrt{\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\
d^2(\mathbf{x}_i, \mathbf{x}_i') = \sum_{j=1}^{p} (x_{ij} - x_{i'j})^2
\end{split}
$$

Outras medidas menos utilizadas incluem a dist√¢ncia absoluta e a dist√¢ncia de Mahalanobis, que leva em considera√ß√£o a matriz de covari√¢ncia, respectivamente representadas como:

$$
\begin{split}
d_a(\mathbf{x}_i, \mathbf{x}_i') = \sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\
d_M(\mathbf{x}_i, \mathbf{x}_i') = \sqrt{(\mathbf{x}_i - \mathbf{x}_i')' \mathbf{S}^{-1} (\mathbf{x}_i - \mathbf{x}_i')}
\end{split}
$$

Uma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados √© por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade $a(x_i,x_j)$ entre cada par de objetos $x_i$ e $x_j$ com $i,j = 1,2,\dots,N.$

$$
\begin{align}
A = 
\begin{bmatrix}
          a(x_1,x_1) & a(x_1,x_2) & \cdots &a(x_1,x_N) \\
         a(x_2,x_1) & a(x_2,x_2) & \cdots & a(x_2,x_N)  \\
            \vdots &\vdots & \ddots &\vdots \\
           a(x_N,x_1) & a(x_N,x_2) & \cdots & a(x_N,x_N)
         \end{bmatrix}.
  \end{align}
$$

As matrizes de dissimilaridade podem ser obtidas com apoio da fun√ß√£o `dist()`, onde o tipo de dist√¢ncia (Euclidiana por exemplo), √© passada no par√¢metro da fun√ß√£o, `method` , veja a seguir, um exemplo aplicado ao conjunto de indicadores obst√©tricos, esse *DataSet* s√©ra o referncial para a sess√£o atual, ser√° considerado apenas as colunas dos indicadores.

```{r eval=FALSE}

dist_euclidian <- dist(scale(dados_indicadores[,-c(1:4)]), method = "euclidean")

```

O c√≥dio acima cria e armazena um objeto do tipo `dist` que ser√° utilizado em exemplos futuros.

A an√°lise de agrupamento √© uma ferramenta valiosa que permite identificar estratos em uma popula√ß√£o e detectar outliers. √â importante considerar a escalabilidade do m√©todo, sua capacidade de lidar com diferentes tipos de vari√°veis e clusters de formatos variados. Al√©m disso, a robustez em rela√ß√£o a outliers e a capacidade de agrupar dados de alta dimensionalidade s√£o considera√ß√µes essenciais. Existem diversos m√©todos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas pr√≥ximas sess√µes, exploraremos os m√©todos considerados e suas aplica√ß√µes adequadas, bem como m√©todos de avalia√ß√£o de qualidade para os agrupamentos.

Neste cap√≠tulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Al√©m disso, temos os m√©todos hier√°rquicos, nos quais os grupos s√£o organizados em uma estrutura de √°rvore.

Outra abordagem interessante √© considerar a densidade dos pontos no espa√ßo. Nesse caso, procuramos identificar regi√µes mais densas separadas por √°reas menos povoadas. Esses m√©todos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na an√°lise dos dados, aqui ser√° considerado o DBSCAN.

Tamb√©m existe uma classe de m√©todos que utiliza t√©cnicas de decomposi√ß√£o espectral. Esses m√©todos reduzem a dimensionalidade dos dados, preservando as informa√ß√µes relevantes dos grupos presentes. S√£o os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.

Cada uma dessas abordagens possui suas pr√≥prias caracter√≠sticas, vantagens e limita√ß√µes.

### M√©todos por Particionamento

Os m√©todos por particionamento s√£o comumente utilizados para agrupar dados, onde cada parti√ß√£o representa um cluster. Esses m√©todos s√£o baseados em dist√¢ncia e envolvem a realoca√ß√£o iterativa das observa√ß√µes entre os clusters para obter um particionamento otimizado.

A escolha do n√∫mero de clusters √© um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum √© o m√©todo do cotovelo, que considera a rela√ß√£o entre a vari√¢ncia total intraclusters e o n√∫mero de grupos criados. O m√©todo do cotovelo considera que aumentar o n√∫mero de clusters reduz a vari√¢ncia, mas em algum ponto, n√£o h√° melhora significativa na granularidade do agrupamento. Esse ponto √≥timo, que indica o n√∫mero adequado de clusters, √© identificado no gr√°fico por uma curva tracejada (veja @fig-screeplot).

![Screeplot para sele√ß√£o de n√∫mero de clusters](figuras_naosupervisionado/cotovelo.png){ #fig-screeplot}

A vari√¢ncia total intraclusters √© calculada utilizando as dist√¢ncias euclidianas quadr√°ticas entre as observa√ß√µes e o centr√≥ide do respectivo grupo. O centr√≥ide $c_l$ de um grupo $C_l$ √© obtido atrav√©s da m√©dia das observa√ß√µes atribu√≠das a esse cluster, utilizando a f√≥rmula:

$$c_l = \frac{1}{|\mathcal{C}_l|} \sum{i \in \mathcal{C}_l} \mathbf{x}_i$$

A vari√¢ncia total intraclusters √© calculada como a soma das dist√¢ncias euclidianas quadr√°ticas entre as observa√ß√µes e os respectivos centr√≥ides, utilizando a f√≥rmula:

$$\sum_{l=1}^{K} \sum_{i \in \mathcal{C}_l} |\mathbf{x}_i - \mathbf{c}_l|^2$$

Essas s√£o algumas das abordagens dos m√©todos por particionamento, aqui ser√° considerado o k-m√©dias e o k-med√≥ides com os algor√≠tmos PAM e CLARA, que ser√£o apresentados a seguir com exemplos de aplica√ß√µes.

#### K-m√©dias

O K-m√©dias √© um m√©todo amplamente utilizado para agrupamento de dados. Ele busca encontrar K parti√ß√µes dos dados, minimizando a vari√¢ncia. O algoritmo de [@lloyd1982least] √© comumente usado para realizar o K-m√©dias. Ele envolve os seguintes passos:

1.  escolha dos K centr√≥ides iniciais;

2.  particionamento dos dados com base na menor dist√¢ncia para cada centr√≥ide;

3.  atualiza√ß√£o dos centr√≥ides com as novas observa√ß√µes atribu√≠das a eles;

4.  repeti√ß√£o dos passos 2 e 3 at√© que n√£o haja mais mudan√ßa de agrupamento. √â poss√≠vel definir um n√∫mero m√°ximo de itera√ß√µes para otimizar o m√©todo computacionalmente.

Uma alternativa √© o algoritmo de [@hartigan1979algorithm], que adiciona uma etapa de valida√ß√£o para alterar os agrupamentos. A cada itera√ß√£o, verifica-se se houve atualiza√ß√£o nos centr√≥ides dos grupos. Nesse caso, um novo objeto s√≥ √© atribu√≠do a um cluster se a soma das dist√¢ncias quadr√°ticas diminuir.

No entanto, o m√©todo K-m√©dias apresenta limita√ß√µes ao lidar com clusters de formas n√£o convencionais ou grupos com tamanhos muito discrepantes. Al√©m disso, ele √© sens√≠vel a outliers, pois a inclus√£o de um dado extremo pode influenciar significativamente o valor do centr√≥ide. A aplica√ß√£o para o software *R*, tanto do m√©todo de agrupamento quanto a escolha do n√∫mero de clusters *K* pelo m√©todo do cotovelo, segue abaixo, ser√° considerado os dados padronizados para retirar qualquer tend√™ncia em fun√ß√£o da diferen√ßa de escala ou amplitude dos dados:

```{r message=FALSE, warning= FALSE, eval=FALSE}
set.seed (1122)
#BIBLIOTECAS
library(ggplot2)
## padronizacao dos dados 

dados_norm <- as.data.frame(scale(dados_indicadores[,-c(1:4)]))

## escolhendo k pelo metodo do cotovelo

cotovelo_kmedias <- factoextra::fviz_nbclust(dados_norm ,
 kmeans,
 method = "wss") +
 geom_vline( xintercept = 7, linetype = 2) +
 labs(x = "Numero de Grupos", y = "Variancia Total Intragrupo", title = "K-medias")

## ajustando k-medias com o numero de grupos escolhido

k_medias <- kmeans(dados_norm,
                        centers = 7)

```

A fun√ß√£o `kmeans` √© uma ferramenta poderosa dispon√≠vel no R para realizar o agrupamento de dados utilizando o m√©todo K-m√©dias. A fun√ß√£o kmeans retorna tr√™s principais objetos:

-   `Cluster_centers`: √â uma matriz que representa os centr√≥ides finais de cada cluster. Cada linha dessa matriz representa um centr√≥ide, com as coordenadas correspondentes √†s vari√°veis do conjunto de dados.

-   `Cluster_assignment`: √â um vetor que cont√©m as atribui√ß√µes de cada observa√ß√£o a um determinado cluster. Cada elemento desse vetor representa o n√∫mero do cluster ao qual a observa√ß√£o foi atribu√≠da. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.

-   `Within_cluster_sum_of_squares`: √â um valor que representa a soma dos quadrados das dist√¢ncias de cada observa√ß√£o em rela√ß√£o ao seu respectivo centr√≥ide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homog√™neo √© o cluster.

#### K-med√≥ides

Em situa√ß√µes com valores extremos, os algoritmos K-med√≥ides surgem como uma alternativa ao c√°lculo do centr√≥ide, evitando a influ√™ncia excessiva desses valores na representa√ß√£o central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por [@kaufman2009finding] considera um custo para as trocas de med√≥ides a cada itera√ß√£o. O custo √© calculado como a diferen√ßa da vari√¢ncia total intragrupo considerando um novo med√≥ide (observa√ß√£o n√£o med√≥ide) em compara√ß√£o com o med√≥ide atual. A vari√¢ncia total intragrupo √© uma medida da dispers√£o dos pontos dentro de um grupo.

Para realizar o agrupamento, o algoritmo PAM segue os seguintes passos:

1.  Escolha inicial dos $K$ med√≥ides a partir do conjunto de dados;

2.  As observa√ß√µes n√£o selecionadas como med√≥ides s√£o atribu√≠das ao grupo cujo med√≥ide √© o mais pr√≥ximo;

3.  Selecionar aleatoriamente uma observa√ß√£o n√£o med√≥ide $o_r$;

4.  Calcular o custo de se mudar o med√≥ide atual para $o_r$;

5.  Caso o custo seja menor que 0, realizar a troca de med√≥ide;

6.  Repetir os passos 2 a 5 at√© que n√£o haja mais mudan√ßas de agrupamento.

O custo de mudan√ßa do med√≥ide atual para outra observa√ß√£o √© calculado como a diferen√ßa da vari√¢ncia total intragrupo considerando a nova observa√ß√£o como representante em compara√ß√£o com o med√≥ide atual.

Al√©m disso, √© comum utilizar a medida de dist√¢ncia absoluta no lugar da dist√¢ncia euclidiana quadr√°tica para calcular a dist√¢ncia entre os pontos e os med√≥ides. O m√©todo pode ser visto abaixo:

```{r eval=FALSE}
## escolhendo k pelo metodo do cotovelo
cotovelo_pam <- factoextra::fviz_nbclust(dados_norm ,
                      cluster::pam,
                      method = "wss") +
                geom_vline( xintercept = 7, linetype = 2) +
                labs(x = "Numero de Grupos",
                      y = "Variancia Total Intragrupo",
                      title = "PAM")

pam <- cluster::pam(dados_norm ,
                      k = 7)

```

A fun√ß√£o `cluster::pam` no *R* retorna os seguintes elementos:

1.  `medoids`: Um vetor contendo os √≠ndices das observa√ß√µes selecionadas como med√≥ides finais de cada cluster.

2.  `clustering`: Um vetor contendo os r√≥tulos dos clusters aos quais cada observa√ß√£o foi atribu√≠da.

3.  `objective`: O valor da medida de dissimilaridade total do agrupamento obtido.

4.  `isolation.distance`: Um vetor com as dist√¢ncias de isolamento de cada observa√ß√£o em rela√ß√£o ao seu med√≥ide correspondente.

5.  `clusinfo`: Uma lista com informa√ß√µes adicionais sobre os clusters, incluindo o n√∫mero de observa√ß√µes em cada cluster e a soma das dist√¢ncias de dissimilaridade intracluster.

Esses elementos fornecem informa√ß√µes sobre os med√≥ides finais selecionados, a atribui√ß√£o de clusters para cada observa√ß√£o, o valor objetivo do agrupamento, as dist√¢ncias de isolamento e informa√ß√µes adicionais sobre cada cluster.

Para lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a vari√¢ncia total intragrupo para cada agrupamento gerado. A parti√ß√£o que apresentar menor vari√¢ncia total intragrupo √© selecionada como o resultado final do algoritmo. Observe abaixo a aplica√ß√£o para o *R*:

```{r eval=FALSE}
cotovelo_clara <- factoextra::fviz_nbclust(dados_norm ,
                      cluster::clara ,
                      method = "wss") +
                  geom_vline( xintercept = 7, linetype = 2) +
                  labs(x = "Numero de Grupos",
                        y = "Variancia Total Intragrupo",
                        title = "CLARA")
clara <- cluster::clara(dados_norm ,
                          k = 7, samples = 10)
```

A fun√ß√£o `cluster::clara` no *R* retorna os seguintes resultados:

1.  `medoids`: Um objeto pamobject contendo os med√≥ides finais de cada cluster.

2.  `clustering`: Um vetor com os r√≥tulos dos clusters atribu√≠dos a cada observa√ß√£o.

3.  `objective`: O valor da medida de dissimilaridade total do agrupamento obtido.

4.  `isolation.distance`: Um vetor com as dist√¢ncias de isolamento de cada observa√ß√£o em rela√ß√£o ao seu med√≥ide correspondente.

5.  `clusinfo`: Uma lista com informa√ß√µes adicionais sobre os clusters, como o n√∫mero de observa√ß√µes em cada cluster e a soma das dist√¢ncias de dissimilaridade intracluster.

6.  `samples`: Uma lista contendo os √≠ndices das observa√ß√µes selecionadas em cada subamostra.

7.  `call`: A chamada original da fun√ß√£o `cluster::clara` que foi utilizada.

Esses resultados fornecem detalhes sobre os med√≥ides finais escolhidos, a atribui√ß√£o dos clusters para cada observa√ß√£o, o valor objetivo do agrupamento, as dist√¢ncias de isolamento, informa√ß√µes adicionais sobre os clusters, as subamostras utilizadas e a chamada original da fun√ß√£o.

### M√©todos Hier√°rquicos

Os m√©todos hier√°rquicos s√£o utilizados para agrupar dados em diferentes n√≠veis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.

Na abordagem aglomerativa, os grupos s√£o constru√≠dos a partir do n√≠vel mais baixo, onde cada observa√ß√£o forma um cluster separado, at√© atingir o n√≠vel mais alto, onde todos os dados est√£o em um √∫nico grupo. A fus√£o dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, tamb√©m conhecidas como linkages, podem ser definidas da seguinte forma:

-   M√©todo do vizinho mais pr√≥ximo (Single linkages): Considera a menor dist√¢ncia entre todas as poss√≠veis combina√ß√µes de observa√ß√µes de dois grupos.

$$
d(C_l,C_{l'}) = \min_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
$$

-   M√©todo do vizinho mais distante (Complete linkages): Utiliza a maior dist√¢ncia entre todas as poss√≠veis combina√ß√µes de observa√ß√µes de dois grupos.

$$
d(C_l,C_{l'}) = \max_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
$$

-   M√©todo da m√©dia das dist√¢ncias (Average linkages): Calcula a m√©dia das dist√¢ncias entre as observa√ß√µes de dois grupos.

$$
d(C_l,C_{l'}) = \frac{1}{|C_l||C_{l'}|}\sum_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
$$

-   M√©todo do centr√≥ide (Centr√≥ide linkages): Considera a dist√¢ncia entre os centr√≥ides de cada grupo como medida de dissimilaridade.

$$
d(C_l,C_{l'}) = d^2(c_l,c_{l'}) .
$$

-   M√©todo de Ward: Minimiza a vari√¢ncia dentro dos grupos ao fundir os clusters que levam √† menor varia√ß√£o poss√≠vel.

$$
d(C_l,C_{l'}) = \frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'}) .
$$

No contexto dos m√©todos hier√°rquicos de agrupamento, a abordagem aglomerativa √© amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada itera√ß√£o √© necess√°rio identificar a melhor divis√£o do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hier√°rquico aglomerativo consiste em:

1.  Cada observa√ß√£o √© inicialmente atribu√≠da a um cluster separado.

2.  Com base no m√©todo de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.

3.  Os dois grupos com a menor dissimilaridade s√£o fundidos em um √∫nico grupo.

4.  Repetem-se os passos 2 e 3 at√© que todas as observa√ß√µes estejam em um √∫nico grupo.

J√° na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo √∫nico que cont√©m todas as observa√ß√µes e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observa√ß√µes. O algoritmo DIANA segue os seguintes passos:

1.  Todas as observa√ß√µes s√£o agrupadas em um √∫nico grupo.

2.  A observa√ß√£o com a maior dissimilaridade m√©dia em rela√ß√£o aos pontos do mesmo grupo √© separada em um novo grupo.

3.  Cada observa√ß√£o do grupo inicial √© atribu√≠da ao novo grupo se a dissimilaridade m√©dia em rela√ß√£o aos objetos desse grupo for menor do que a dissimilaridade m√©dia em rela√ß√£o aos demais pontos do grupo inicial.

4.Calcula-se o di√¢metro de todos os grupos (a maior dissimilaridade entre duas observa√ß√µes) e seleciona-se o grupo com o maior di√¢metro.

5.  Repetem-se os passos 2 a 4 at√© que cada observa√ß√£o esteja em um grupo separado.

No agrupamento hier√°rquico, a visualiza√ß√£o dos clusters √© feita por meio de um dendrograma, um gr√°fico ramificado que mostra as jun√ß√µes e divis√µes dos clusters. A altura do ramo no primeiro n√≥ do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o n√∫mero de grupos a partir do dendrograma, busca-se uma grande diferen√ßa de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma caracter√≠stica dos m√©todos hier√°rquicos √© que as decis√µes de agrupamento ou divis√£o n√£o s√£o desfeitas, ou seja, n√£o h√° troca de observa√ß√µes entre os clusters. Decis√µes de uni√£o ou divis√£o mal feitas podem resultar em grupos de baixa qualidade. Al√©m disso, esses m√©todos n√£o s√£o bem dimensionados, pois cada decis√£o de mesclagem ou divis√£o requer a avalia√ß√£o de muitos objetos ou clusters. Pelo exemplo abaixo na @fig-dendrograma, uma poss√≠vel resposta de n√∫mero adequado de clusters seria de dois ou tr√™s grupos.

![Exemplo de Dendrograma](figuras_naosupervisionado/dendograma.png){#fig-dendrograma}

Os agrupamentos hier√°rquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das fun√ß√µes `hclust` e `cluster::diana`, √© obtido um objeto da classe "hclust" e da clase "diana" respectivamente, esses objetos cont√©m informa√ß√µes sobre o agrupamento hier√°rquico realizado, incluindo a estrutura do dendrograma, as dist√¢ncias entre os objetos e outras propriedades relacionadas. Seguido pela sele√ß√£o, com base no dendrograma, do n√∫mero *K* ideal de clusters (N√£o necessariamente s√≥ um *K*), e o "corte" da √°rvore aglomerativa no valor ideal identificado. √â apresentado abaixo para todos os m√©todos citados a aplica√ß√£o para o *R*, supondo a dist√¢ncia utilizada como a euclidiana calculada no in√≠cio do cap√≠tulo.

```{r eval=FALSE}
library(ggdendro)
#AGLOMERATIVOS ############
#METODO WARD -----
##CRIAR O OBJETO HCLUST
agl_ward <- hclust(dist_euclidian , method = "ward.D2")

# PLOT DO DENDROGRAMA 
dendograma_ward <-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,
 main = "Ward - cortado em H = 20")

# "corte" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS
agl_ward_res <- cutree(agl_ward , k = 3:8)

# segue o mesmo para todos os outros metodos

#METODO SINGLE LINKAGE -----
agl_single <- hclust(dist_euclidian , method = "single")

dendograma_single <- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,
                        main = "Vizinho mais Proximo - cortado em H = 4",
                          xlab = "")

agl_single_res <- cutree(agl_single , k = 3:8)

#METODO COMPLETE LINKAGE -----
agl_complete <-  hclust(dist_euclidian , method = "complete")

dendograma_complete <- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,
                             main = "Vizinho mais Distante - cortado em H = 10")

agl_complete_res <- cutree(agl_complete , k = 3:8)

#METODO AVARAGE LINKAGE -----
agl_ave <- hclust(dist_euclidian , method = "average")

dendograma_ward <- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,
                                main = " Media das Distancias - cortado em H = 15")

agl_ave_res <- cutree(agl_ave, k = 3:8)

#METODO CENTROIDE LINKAGE -----
agl_cent <- hclust(dist_euclidian , method = "centroid")

dendograma_ward <- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,
                            main = " Centroide - cortado em H = 15")

agl_cent_res <- cutree(agl_cent , k = 3:8)

####### DIVISIVO (DIANA) #########
diana <- cluster::diana(dist_euclidian ,
                            diss = TRUE ,
                            metric = "euclidean",
                            keep.diss = FALSE ,
                            keep.data = FALSE
                            )

dendrograma_diana <-  plot(cut(as.dendrogram(diana), h = 7)$upper ,
                                   main = "Diana - cortado em H = 7")

diana_res <- cutree(diana , k = 3:8)
```

### DBSCAN

O m√©todo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) √© um algoritmo de agrupamento que foi proposto para encontrar clusters com formas arbitr√°rias, ou seja, clusters que n√£o necessariamente possuem uma forma esf√©rica ou convexa. Ele busca identificar as regi√µes mais densas do espa√ßo vetorial separadas por regi√µes com menos objetos, como √© poss√≠vel ver na @fig-dbscan sua efici√™ncia em rela√ß√£o a outros m√©todos.

![Aplica√ß√£o DBSCAN, Fonte: Boyke et al. (2021)](figuras_naosupervisionado/dbscan.png){#fig-dbscan}

A ideia geral do algoritmo DBSCAN √© identificar os clusters de forma que a densidade de pontos ao redor de cada ponto de um grupo seja maior que um limite estabelecido. Ele utiliza dois par√¢metros principais: $\epsilon$ e MinPts, para entendimento do agrupamento DBSCAN observe a defini√ß√£o dos seguintes termos:

-   $\epsilon$-vizinhos: Os $\epsilon$-vizinhos de um ponto $x_j$ s√£o os objetos cuja dist√¢ncia para $x_i$ seja menor ou igual a $\epsilon$, ou seja, a vizinhan√ßa de $x_i$ √© composta pelos objetos que est√£o dentro de um raio $\epsilon$ ao redor de $x_i$.

-   Ponto de N√∫cleo: Um ponto $x_i$ √© considerado um ponto de n√∫cleo se o n√∫mero de seus $\epsilon$-vizinhos (ou seja, os pontos dentro da vizinhan√ßa de $x_i$) for maior ou igual a um valor m√≠nimo estabelecido chamado MinPts.

-   Ponto de Borda: Um ponto $x_i$ √© considerado um ponto de borda se o n√∫mero de seus $\epsilon$ -vizinhos for menor do que MinPts, mas ele est√° dentro da vizinhan√ßa de algum ponto de n√∫cleo.

-   Diretamente alcan√ß√°vel por densidade: Um ponto $x_j$ √© diretamente alcan√ß√°vel por densidade por um ponto $x_i$ se $x_j$ √© um $\epsilon$-vizinho de $x_i$ e se $x_i$ √© um ponto de n√∫cleo.

-   Alcan√ß√°vel por densidade: Um ponto $x_i$ √© alcan√ß√°vel por densidade por um ponto $x_j$ se existe uma cadeia de pontos $x_{i'}$ , onde \$i' \leq N \in \\mathbb{N} \$, em que $x_1 = x_j, x_N = x_i$ e $x_{i' + 1}$ √© diretamente alcan√ß√°vel por densidade por $x_{i'}$.

-   Conectado por densidade: Um ponto $x_i$ √© conectado por densidade a um ponto $x_j$ se existe um terceiro ponto $x_{i'}$ em que ambos s√£o alcan√ß√°veis por densidade por $x_{i'}$.

Com base nesses crit√©rios, o DBSCAN define os grupos da seguinte forma:

-   Se um ponto $x_i$ √© alcan√ß√°vel por densidade por um ponto $x_j$, ent√£o ambos pertencem ao mesmo cluster $C_l$.

-   Todos os pontos de um cluster s√£o conectados por densidade entre si.

Uma das vantagens do DBSCAN √© sua capacidade de identificar outliers no conjunto de dados, uma vez que ele n√£o atribui todos os objetos a grupos. Os outliers s√£o definidos como os pontos que n√£o s√£o atribu√≠dos a nenhum cluster. O algoritmo DBSCAN funciona da seguinte maneira:

1.  Inicialmente, todas as observa√ß√µes da base de dados s√£o classificadas como "n√£o visitadas".

2.  Em seguida, os seguintes passos s√£o executados repetidamente:

    (a). Selecionar aleatoriamente uma observa√ß√£o "n√£o visitada" $x_i$.

    (b). Verificar o n√∫mero de $epsilon$-vizinhos da observa√ß√£o $x_i$ selecionada:

    -   Se o n√∫mero de $\epsilon$-vizinhos $|NEps(x_i)|$ for maior ou igual a MinPts, um novo cluster √© criado para $x_i$, e todos os objetos na $\epsilon$-vizinhan√ßa de $x_i$ s√£o adicionados a um conjunto candidato, $M$. O algoritmo adiciona iterativamente a $C_l$ todos os objetos em ùëÄ que n√£o pertencem a nenhum cluster.

    -   Se o n√∫mero de $\epsilon$-vizinhos $|NEps(x_i)|$ for menor do que MinPts, $x_i$ √© marcado como um ponto de ru√≠do (outlier).

3.  Repetir os passos 1 e 2 at√© que n√£o haja mais observa√ß√µes "n√£o visitadas".

Os par√¢metros $\epsilon$ e MinPts afetam diretamente o resultado do agrupamento, determinando o tamanho m√≠nimo de cada grupo e a dist√¢ncia entre os clusters. Uma das limita√ß√µes do DBSCAN √© o fato de que esses par√¢metros s√£o globais, ou seja, definidos para todos os grupos formados, o que pode n√£o ser ideal quando diferentes clusters possuem densidades de pontos distintas. Para a sele√ß√£o do valor de MinPts √© comum que seja baseado no problema em si ou conhecimento pr√©vio. Por√©m, uma proposta para sele√ß√£o √© o MinPts = 2√ó$p$ pode ser um bom valor para o par√¢metro. J√° o $\epsilon$ costuma ser baseado no n√∫mero m√≠nimo de pontos definido, em que √© constru√≠do o gr√°fico das dist√¢ncias de cada ponto para os MinPts‚àí1 vizinhos mais pr√≥ximos e escolhido o valor "cotovelo", ou seja, onde come√ßa a ocorrer um crescimento mais acentuado nas dist√¢ncias ordenadas. Abaixo vemos como esse m√©todo se d√° de forma aplicada para o software de programa√ß√£o tanto da sele√ß√£o do melhor $\epsilon$ quanto da aplica√ß√£o do modelo.

```{r eval=FALSE}
## selecao do melhor eps
 dbscan::kNNdist(dados_diss ,
 k = 3, all = FALSE)
## ajuste com o eps selecionado
 dbscan <- dbscan::dbscan(dados_norm , eps = 2, minPts = 4)
```

Essa fun√ß√£o retorna os seguintes elementos:

-   `cluster`: √â um vetor que atribui um n√∫mero de cluster a cada ponto de dados no conjunto de entrada. Os pontos que n√£o pertencem a nenhum cluster s√£o rotulados como "0" ou como "outlier".

-   `eps`: √â o valor do raio (epsilon) usado no algoritmo.

-   `minPts`: √â o n√∫mero m√≠nimo de pontos necess√°rios para formar um cluster.

-   `border`: √â um vetor l√≥gico que indica se cada ponto √© um ponto de borda.

-   `reachability`: √â um vetor que armazena os valores de *reachability distance* de cada ponto.

-   `core_dist`: √â um vetor que cont√©m as dist√¢ncias de densidade m√≠nima( *core distance* ) para cada ponto.

Uma t√©cnica relacionada ao DBSCAN que lida com a limita√ß√£o dos par√¢metros globais √© o algoritmo OPTICS (Ordering Points To Identify the Clustering Structure), proposto por [@ankerst1999optics]. O OPTICS ordena os dados de forma que as observa√ß√µes em grupos mais densos estar√£o mais pr√≥ximas na ordena√ß√£o. Esse algoritmo pode ser uma op√ß√£o para explorar diferentes densidades de pontos em diferentes clusters.

### Agrupamento Espectral

Agrupar conjuntos de dados com alta dimensionalidade pode ser desafiador para alguns m√©todos de agrupamento. A grande quantidade de vari√°veis em alta dimens√£o pode aumentar significativamente o custo computacional e dificultar a separa√ß√£o adequada dos grupos. Nesse contexto, os m√©todos de agrupamento espectral surgem como uma abordagem promissora, pois permitem reduzir a dimensionalidade dos dados sem perder a informa√ß√£o contida nas vari√°veis, al√©m de melhorar a separa√ß√£o entre grupos que podem n√£o estar claramente distintos devido √† alta dimensionalidade.

Os algoritmos de agrupamento espectral usam uma medida de dissimilaridade para representar o conjunto de dados como um grafo. Nessa representa√ß√£o, os v√©rtices do grafo correspondem √†s observa√ß√µes ($V = v_1, ..., v_N$), e as arestas ($A$) s√£o definidas por uma matriz de similaridade, onde $a(x_i, x_{i'})$ representa o peso da aresta que conecta os v√©rtices $x_i$ e $x_{i'}$. O agrupamento em $K$ clusters √© ent√£o formulado como um problema de corte de arestas, que pode ser computacionalmente complexo. Para contornar essa complexidade, √© aplicada a teoria espectral.

O algoritmo de Ng, Jordan e Weiss (NJW) √© um exemplo de algoritmo de agrupamento espectral. Ele requer a defini√ß√£o pr√©via de um par√¢metro, assim como o algoritmo K-m√©dias. Esse par√¢metro √© o n√∫mero de grupos $K$ a serem formados. O algoritmo NJW segue os seguintes passos:

1.  Calcula-se a matriz de similaridade $A$, usando um par√¢metro escalar $\sigma$. A similaridade entre cada par de pontos $x_i$ e $x_{i'}$ √© medida usando uma fun√ß√£o de similaridade, como a fun√ß√£o Gaussiana. A matriz $A$ √© preenchida com os valores calculados.

2.  Calcula-se a matriz de graus $D$, onde cada elemento $d_{ij}$ na diagonal principal representa a soma das similaridades da linha $i$ da matriz $A$. A matriz $D$ captura a estrutura de conectividade do conjunto de dados.

3.  Constr√≥i-se a matriz Laplaciana $L$, que √© uma forma de normaliza√ß√£o de $A$. No algoritmo NJW, a matriz Laplaciana √© definida como $L = D^{(-1/2)} ùê¥ ùê∑^{(-1/2)}$. Essa normaliza√ß√£o real√ßa as diferen√ßas entre os grupos de dados.

4.  Identificam-se os $K$ maiores autovalores de $L$ e seus respectivos autovetores associados, que s√£o armazenados em uma matriz $Z$ de tamanho $N \times K$.

5.  Define-se uma nova matriz $Y$, normalizada a partir de $Z$. Cada elemento $y_{ii'}$ de $Y$ √© calculado dividindo-se o elemento $z_{ii'}$ de $Z$ pelo produto da raiz quadrada do elemento diagonal correspondente a $z_{ii}$ e a raiz quadrada do elemento diagonal correspondente a $z_{i'}$. Essa normaliza√ß√£o ajusta as magnitudes dos dados.

6.  Com a redu√ß√£o da dimensionalidade dos dados na matriz $Y$ (de tamanho $N \times K$), pode-se aplicar um algoritmo de agrupamento, como o K-m√©dias, para realizar o agrupamento dos dados.

```{r eval=FALSE}
spectral_clustering <- function(data , # dados
                                k) # numero de grupos
                                {
    # matriz de dissimilaridade
    A <- as.matrix(KRLS::gausskernel (data , 2)) # sigma^2 = 1
    diag(A) <- 0
    
    # matriz Laplaciana
    L = diag(1 / sqrt(rowSums(A))) %*% A %*% diag(1 / sqrt(rowSums(A)))
    
    # matriz de autovetores associados aos k maiores autovalores
    auto <- eigen(L, symmetric = TRUE)
    X <- auto$vectors[, 1:k]
    
    # matriz normalizada
    Y <- X / sqrt(rowSums(X ^ 2))
    
    # agrupamente k medias na matriz redimensionalizada
    set.seed (1122)
    k_means <- kmeans(Y, centers = k)
    
    return(list(
    clusters = k_means$cluster , #retornar grupos
    auto = auto , #retornar autovalores e autovetores
    autovetores = X, #retornar autovetores dos K maiores autovalores
    normalizada = Y #retornar matriz normalizada
    ))
}

```

A escolha dos par√¢metros $K$ e $\sigma$ pode ser feita de forma arbitr√°ria, dependendo do objetivo do estudo. No entanto, existem abordagens e t√©cnicas dispon√≠veis para auxiliar na sele√ß√£o desses par√¢metros. No exemplo acima , o par√¢metro $\sigma$ foi fixado em 1, e o n√∫mero de grupos $K$ foi escolhido com base na an√°lise dos primeiros autovalores da matriz Laplaciana $L$. O valor √≥timo de $K$ √© aquele em que ocorre uma queda significativa dos autovalores, uma vez que os maiores autovalores da matriz Laplaciana tendem a ser pr√≥ximos de 1.

### Valida√ß√£o do Modelo

No problema do agrupamento, n√£o √© poss√≠vel verificar o grau de acerto do resultado obtido, uma vez que os verdadeiros grupos n√£o s√£o conhecidos a priori. Portanto, √© necess√°rio aplicar algum tipo de valida√ß√£o √† parti√ß√£o final.

Quatro √≠ndices de avalia√ß√£o interna da qualidade do agrupamento s√£o os principais a serem utilizados atualmente: Davies-Bouldin (DB), Dunn (D), Silhueta (S) e Calinski-Harabasz (CH).

#### Davies-Bouldin (DB):

O √≠ndice DB mede a similaridade m√©dia entre cada grupo e seu grupo mais similar dentre os demais clusters. A dist√¢ncia m√©dia $\delta_l$ de um grupo $l$ √†s suas observa√ß√µes √© calculada em rela√ß√£o a um valor referencial $m_l$. A f√≥rmula para $\delta_l$ √©:

$$
\delta_{l} = \left(\frac{1}{n_{l}} \sum_{i=1}^{n_{l}} \left \| x_{i} - m_{l} \right \|^q\right)^{\frac{1}{q}}.
$$

onde $n_l$ √© o n√∫mero de observa√ß√µes no grupo $l$, $x_i$ √© uma observa√ß√£o, $m_l$ √© o valor referencial (centr√≥ide ou med√≥ide) e $q$ √© um valor pr√©-definido, onde os valores mais utilizados s√£o $q = 1$ e $q= 2$. Para o √≠ndice, √© utilizada tamb√©m dist√¢ncia entre os grupos $\Delta_{ll'}$, que √© obtida como a dist√¢ncia entre os valores referenciais de cada grupo. A f√≥rmula para $\Delta_{ll'}$ √©:

$$
\Delta_{ll'} = \left( \sum_{j=1}^{p} |m_{jl} - m_{jl'}|^{t} \right)^{\frac{1}{t}}.
$$

onde $t \in \mathbb{N}$ √© pr√©-definido e geralmente adotado como $t = 1$ (dist√¢ncias absolutas) ou $t=2$ (dist√¢ncia euclidiana). Assim, o √≠ndice de Davies-Bouldin (DB) fica definido por:

$$
DB = \frac{1}{K} \sum_{l=1}^{K} \max_{l \neq l'} \left( \frac{\delta_{l} + \delta_{l'}}{\Delta_{ll'}} \right).
$$ Nesse caso, buscamos agrupar observa√ß√µes de forma a minimizar a vari√¢ncia intragrupo e maximizar a diferen√ßa entre eles. Portanto, valores menores do √≠ndice de Davies-Bouldin s√£o considerados melhores. Observe a m√©trica aplicada ao conjunto de dados de agrupamento obtido pelo k-m√©dias (As medidas seguintes tamb√©m ser√£o aplicadas ao mesmo conjunto).

```{r eval=FALSE}
#Metrica DB usando Centroide
clusterSim::index.DB(dados_norm ,
                     k_medias$cluster)$DB
#Metrica DB usando Medoide
clusterSim::index.DB(dados_norm ,
                      k_medias$cluster ,
                      d = dist_euclidian ,
                      centrotypes = "medoids"
                      )$DB 

```

#### Dunn (D):

O √≠ndice D mede a raz√£o entre a separa√ß√£o dos grupos e a vari√¢ncia dentro deles. A separa√ß√£o entre dois grupos $l$ e $l'$ √© calculada pela dist√¢ncia do vizinho mais pr√≥ximo, dessa forma buscamos valores grandes para essa medida de avalia√ß√£o. A vari√¢ncia intragrupo √© representada pelo di√¢metro $diam_l$ do cluster. A f√≥rmula para o √≠ndice D √©:

$$
D = \frac{{\min_{l,l' \in \{1,...,K\}; l \neq l'} d(C_{l}, C_{l'})}}{{\max_{l \in \{1,...,K\}} diam_{l}}}.
$$

```{r eval=FALSE}
#Metrica D
 clValid::dunn(distance = dist_euclidian , k_medias$cluster)
```

#### Silhueta (S):

O √≠ndice de Silhueta(S) mede a qualidade do agrupamento considerando as dist√¢ncias de cada ponto em rela√ß√£o √†s observa√ß√µes do mesmo grupo e aos demais clusters formados. Para cada observa√ß√£o $i$ pertencente ao grupo $C_l$, definimos as medidas $a_i$ e $b_i$ da seguinte maneira:

\$\$

```{=tex}
\begin{split}
a_i = \frac{1}{n_{l}-1} \sum_{i' \neq i, i' \in C_{l}}^{n_l} d(x_{i}, x_{i'}),\\

b_i = \min_{l \neq l'} \left( \frac{1}{n_{l'}} \sum_{i' \in C_{l'}}^{n_{l'}} d(x_{i}, x_{i'}) \right).
\end{split}
```
\$\$

onde $d(x_i,x_{i'})$ representa a dist√¢ncia entre as observa√ß√µes $x_i$ e $x_{i'}$. A partir dessas medidas, calculamos a silhueta $s_i$ da observa√ß√£o $i$, com $i = 1, ..., N$, usando a f√≥rmula:

$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}.
$$

O coeficiente de Silhueta S global √© obtido por:

$$
S = \frac{1}{K} \sum_{l=1}^{K} \frac{1}{n_{l}} \sum_{i=1}^{n_{l}} s_i.
$$ Dessa forma, valores maiores de S indicam agrupamentos mais densos e separados, o que √© considerado um cen√°rio adequado para um agrupamento bem-sucedido.

```{r eval=FALSE}
#Metrica S
clusterSim::index.S(dist_euclidian , k_medias$cluster)
```

#### Calinski-Harabasz (CH):

O √≠ndice de Calinski-Harabasz (CH) considera a vari√¢ncia intra-grupo, chamada de $WGGS_l$, de cada cluster $C_l$ gerado, levando em conta a dist√¢ncia quadr√°tica de cada observa√ß√£o em rela√ß√£o ao seu valor de refer√™ncia $m_l$, que pode ser um centr√≥ide ou med√≥ide. A vari√¢ncia intra-grupo total,$WGGS$, √© calculada como a soma das vari√¢ncias intra-grupo de cada cluster:

$$
WGGS = \sum_{l=1}^{K} \sum_{i=1}^{n_{l}} d^2(x_{i}, m_l).
$$

Al√©m disso, o c√°lculo do √≠ndice CH utiliza uma medida de dispers√£o $BGSS$ entre os grupos, que √© obtida atrav√©s da soma ponderada das dist√¢ncias quadr√°ticas do valor de refer√™ncia de cada cluster em rela√ß√£o a um valor central global $m$. Essa medida √© calculada da seguinte forma:

$$
BGSS = \sum_{l=1}^{K} n_{l} d^2(m_l, m).
$$

Dessa forma, o √≠ndice CH de Calinski-Harabasz √© dado por:

$$
CH = \frac{(N - K) }{K - 1}\times \frac{BGSS}{WGSS}.
$$

Onde, valores maiores do √≠ndice CH indicam grupos com menor vari√¢ncia e bem separados, o que √© considerado uma boa caracter√≠stica de um agrupamento.

```{r eval=FALSE}

#Metrica CH usando Medoide
clusterSim::index.G1(dados_norm , k_medias$cluster , 
                     d = dist_euclidian , centrotypes = "medoids")

#Metrica CH usando centroide
clusterSim::index.G1(dados_norm , k_medias$cluster)
```
