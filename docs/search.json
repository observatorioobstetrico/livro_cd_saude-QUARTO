[
  {
    "objectID": "naosupervisionado.html#an√°lise-de-agrupamentos",
    "href": "naosupervisionado.html#an√°lise-de-agrupamentos",
    "title": "9¬† Aprendizado N√£o Supervisionado",
    "section": "9.1 An√°lise de Agrupamentos",
    "text": "9.1 An√°lise de Agrupamentos\nComo descrito anteriormente e refor√ßado aqui, na an√°lise de agrupamento, buscamos identificar regi√µes no espa√ßo dos dados que possuam um grande n√∫mero de observa√ß√µes pr√≥ximas umas das outras. Essas regi√µes s√£o chamadas de clusters. A ideia √© agrupar indiv√≠duos que sejam semelhantes entre si e diferentes dos indiv√≠duos em outros clusters. Essa t√©cnica √© chamada de aprendizado n√£o supervisionado, pois n√£o utilizamos uma vari√°vel espec√≠fica como refer√™ncia para avaliar o resultado do agrupamento.\nFormalmente, os clusters s√£o definidos da seguinte forma:\n\nCada cluster √© um grupo de observa√ß√µes;\nTodos os indiv√≠duos pertencem a pelo menos um cluster;\nDois clusters diferentes n√£o possuem observa√ß√µes em comum.\n\nAo realizar o agrupamento de dados, √© importante utilizar um m√©todo que maximize as diferen√ßas entre os clusters, ao mesmo tempo que minimiza as diferen√ßas dentro de cada cluster. Para isso, s√£o utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferen√ßas entre as observa√ß√µes.\nAs medidas de dissimilaridade mais comumente usadas s√£o a dist√¢ncia euclidiana e a dist√¢ncia euclidiana quadr√°tica, como apresentado abaixo respectivamente:\n\\[\n\\begin{split}\nd(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\\\\nd^2(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2\n\\end{split}\n\\]\nOutras medidas menos utilizadas incluem a dist√¢ncia absoluta e a dist√¢ncia de Mahalanobis, que leva em considera√ß√£o a matriz de covari√¢ncia, respectivamente representadas como:\n\\[\n\\begin{split}\nd_a(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\\\\nd_M(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_i')' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_i')}\n\\end{split}\n\\]\nUma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados √© por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade \\(a(x_i,x_j)\\) entre cada par de objetos \\(x_i\\) e \\(x_j\\) com \\(i,j = 1,2,\\dots,N.\\)\n\\[\n\\begin{align}\nA =\n\\begin{bmatrix}\n          a(x_1,x_1) & a(x_1,x_2) & \\cdots &a(x_1,x_N) \\\\\n         a(x_2,x_1) & a(x_2,x_2) & \\cdots & a(x_2,x_N)  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           a(x_N,x_1) & a(x_N,x_2) & \\cdots & a(x_N,x_N)\n         \\end{bmatrix}.\n  \\end{align}\n\\]\nAs matrizes de dissimilaridade podem ser obtidas com apoio da fun√ß√£o dist(), onde o tipo de dist√¢ncia (Euclidiana por exemplo), √© passada no par√¢metro da fun√ß√£o, method , veja a seguir, um exemplo aplicado ao conjunto de indicadores obst√©tricos, esse DataSet s√©ra o referncial para a sess√£o atual, ser√° considerado apenas as colunas dos indicadores.\n\ndist_euclidian &lt;- dist(scale(dados_indicadores[,-c(1:4)]), method = \"euclidean\")\n\nO c√≥dio acima cria e armazena um objeto do tipo dist que ser√° utilizado em exemplos futuros.\nA an√°lise de agrupamento √© uma ferramenta valiosa que permite identificar estratos em uma popula√ß√£o e detectar outliers. √â importante considerar a escalabilidade do m√©todo, sua capacidade de lidar com diferentes tipos de vari√°veis e clusters de formatos variados. Al√©m disso, a robustez em rela√ß√£o a outliers e a capacidade de agrupar dados de alta dimensionalidade s√£o considera√ß√µes essenciais. Existem diversos m√©todos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas pr√≥ximas sess√µes, exploraremos os m√©todos considerados e suas aplica√ß√µes adequadas, bem como m√©todos de avalia√ß√£o de qualidade para os agrupamentos.\nNeste cap√≠tulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Al√©m disso, temos os m√©todos hier√°rquicos, nos quais os grupos s√£o organizados em uma estrutura de √°rvore.\nOutra abordagem interessante √© considerar a densidade dos pontos no espa√ßo. Nesse caso, procuramos identificar regi√µes mais densas separadas por √°reas menos povoadas. Esses m√©todos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na an√°lise dos dados, aqui ser√° considerado o DBSCAN.\nTamb√©m existe uma classe de m√©todos que utiliza t√©cnicas de decomposi√ß√£o espectral. Esses m√©todos reduzem a dimensionalidade dos dados, preservando as informa√ß√µes relevantes dos grupos presentes. S√£o os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.\nCada uma dessas abordagens possui suas pr√≥prias caracter√≠sticas, vantagens e limita√ß√µes.\n\n9.1.1 M√©todos por Particionamento\nOs m√©todos por particionamento s√£o comumente utilizados para agrupar dados, onde cada parti√ß√£o representa um cluster. Esses m√©todos s√£o baseados em dist√¢ncia e envolvem a realoca√ß√£o iterativa das observa√ß√µes entre os clusters para obter um particionamento otimizado.\nA escolha do n√∫mero de clusters √© um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum √© o m√©todo do cotovelo, que considera a rela√ß√£o entre a vari√¢ncia total intraclusters e o n√∫mero de grupos criados. O m√©todo do cotovelo considera que aumentar o n√∫mero de clusters reduz a vari√¢ncia, mas em algum ponto, n√£o h√° melhora significativa na granularidade do agrupamento. Esse ponto √≥timo, que indica o n√∫mero adequado de clusters, √© identificado no gr√°fico por uma curva tracejada (veja Figura¬†9.1).\n\n\n\nFigura¬†9.1: Screeplot para sele√ß√£o de n√∫mero de clusters\n\n\nA vari√¢ncia total intraclusters √© calculada utilizando as dist√¢ncias euclidianas quadr√°ticas entre as observa√ß√µes e o centr√≥ide do respectivo grupo. O centr√≥ide \\(c_l\\) de um grupo \\(C_l\\) √© obtido atrav√©s da m√©dia das observa√ß√µes atribu√≠das a esse cluster, utilizando a f√≥rmula:\n\\[c_l = \\frac{1}{|\\mathcal{C}_l|} \\sum{i \\in \\mathcal{C}_l} \\mathbf{x}_i\\]\nA vari√¢ncia total intraclusters √© calculada como a soma das dist√¢ncias euclidianas quadr√°ticas entre as observa√ß√µes e os respectivos centr√≥ides, utilizando a f√≥rmula:\n\\[\\sum_{l=1}^{K} \\sum_{i \\in \\mathcal{C}_l} |\\mathbf{x}_i - \\mathbf{c}_l|^2\\]\nEssas s√£o algumas das abordagens dos m√©todos por particionamento, aqui ser√° considerado o k-m√©dias e o k-med√≥ides com os algor√≠tmos PAM e CLARA, que ser√£o apresentados a seguir com exemplos de aplica√ß√µes.\n\n9.1.1.1 K-m√©dias\nO K-m√©dias √© um m√©todo amplamente utilizado para agrupamento de dados. Ele busca encontrar K parti√ß√µes dos dados, minimizando a vari√¢ncia. O algoritmo de (Lloyd 1982) √© comumente usado para realizar o K-m√©dias. Ele envolve os seguintes passos:\n\nescolha dos K centr√≥ides iniciais;\nparticionamento dos dados com base na menor dist√¢ncia para cada centr√≥ide;\natualiza√ß√£o dos centr√≥ides com as novas observa√ß√µes atribu√≠das a eles;\nrepeti√ß√£o dos passos 2 e 3 at√© que n√£o haja mais mudan√ßa de agrupamento. √â poss√≠vel definir um n√∫mero m√°ximo de itera√ß√µes para otimizar o m√©todo computacionalmente.\n\nUma alternativa √© o algoritmo de (Hartigan e Wong 1979), que adiciona uma etapa de valida√ß√£o para alterar os agrupamentos. A cada itera√ß√£o, verifica-se se houve atualiza√ß√£o nos centr√≥ides dos grupos. Nesse caso, um novo objeto s√≥ √© atribu√≠do a um cluster se a soma das dist√¢ncias quadr√°ticas diminuir.\nNo entanto, o m√©todo K-m√©dias apresenta limita√ß√µes ao lidar com clusters de formas n√£o convencionais ou grupos com tamanhos muito discrepantes. Al√©m disso, ele √© sens√≠vel a outliers, pois a inclus√£o de um dado extremo pode influenciar significativamente o valor do centr√≥ide. A aplica√ß√£o para o software R, tanto do m√©todo de agrupamento quanto a escolha do n√∫mero de clusters K pelo m√©todo do cotovelo, segue abaixo, ser√° considerado os dados padronizados para retirar qualquer tend√™ncia em fun√ß√£o da diferen√ßa de escala ou amplitude dos dados:\n\nset.seed (1122)\n#BIBLIOTECAS\nlibrary(ggplot2)\n## padronizacao dos dados \n\ndados_norm &lt;- as.data.frame(scale(dados_indicadores[,-c(1:4)]))\n\n## escolhendo k pelo metodo do cotovelo\n\ncotovelo_kmedias &lt;- factoextra::fviz_nbclust(dados_norm ,\n kmeans,\n method = \"wss\") +\n geom_vline( xintercept = 7, linetype = 2) +\n labs(x = \"Numero de Grupos\", y = \"Variancia Total Intragrupo\", title = \"K-medias\")\n\n## ajustando k-medias com o numero de grupos escolhido\n\nk_medias &lt;- kmeans(dados_norm,\n                        centers = 7)\n\nA fun√ß√£o kmeans √© uma ferramenta poderosa dispon√≠vel no R para realizar o agrupamento de dados utilizando o m√©todo K-m√©dias. A fun√ß√£o kmeans retorna tr√™s principais objetos:\n\nCluster_centers: √â uma matriz que representa os centr√≥ides finais de cada cluster. Cada linha dessa matriz representa um centr√≥ide, com as coordenadas correspondentes √†s vari√°veis do conjunto de dados.\nCluster_assignment: √â um vetor que cont√©m as atribui√ß√µes de cada observa√ß√£o a um determinado cluster. Cada elemento desse vetor representa o n√∫mero do cluster ao qual a observa√ß√£o foi atribu√≠da. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.\nWithin_cluster_sum_of_squares: √â um valor que representa a soma dos quadrados das dist√¢ncias de cada observa√ß√£o em rela√ß√£o ao seu respectivo centr√≥ide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homog√™neo √© o cluster.\n\n\n\n9.1.1.2 K-med√≥ides\nEm situa√ß√µes com valores extremos, os algoritmos K-med√≥ides surgem como uma alternativa ao c√°lculo do centr√≥ide, evitando a influ√™ncia excessiva desses valores na representa√ß√£o central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por (Kaufman e Rousseeuw 2009) considera um custo para as trocas de med√≥ides a cada itera√ß√£o. O custo √© calculado como a diferen√ßa da vari√¢ncia total intragrupo considerando um novo med√≥ide (observa√ß√£o n√£o med√≥ide) em compara√ß√£o com o med√≥ide atual. A vari√¢ncia total intragrupo √© uma medida da dispers√£o dos pontos dentro de um grupo.\nPara realizar o agrupamento, o algoritmo PAM segue os seguintes passos:\n\nEscolha inicial dos \\(K\\) med√≥ides a partir do conjunto de dados;\nAs observa√ß√µes n√£o selecionadas como med√≥ides s√£o atribu√≠das ao grupo cujo med√≥ide √© o mais pr√≥ximo;\nSelecionar aleatoriamente uma observa√ß√£o n√£o med√≥ide \\(o_r\\);\nCalcular o custo de se mudar o med√≥ide atual para \\(o_r\\);\nCaso o custo seja menor que 0, realizar a troca de med√≥ide;\nRepetir os passos 2 a 5 at√© que n√£o haja mais mudan√ßas de agrupamento.\n\nO custo de mudan√ßa do med√≥ide atual para outra observa√ß√£o √© calculado como a diferen√ßa da vari√¢ncia total intragrupo considerando a nova observa√ß√£o como representante em compara√ß√£o com o med√≥ide atual.\nAl√©m disso, √© comum utilizar a medida de dist√¢ncia absoluta no lugar da dist√¢ncia euclidiana quadr√°tica para calcular a dist√¢ncia entre os pontos e os med√≥ides. O m√©todo pode ser visto abaixo:\n\n## escolhendo k pelo metodo do cotovelo\ncotovelo_pam &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::pam,\n                      method = \"wss\") +\n                geom_vline( xintercept = 7, linetype = 2) +\n                labs(x = \"Numero de Grupos\",\n                      y = \"Variancia Total Intragrupo\",\n                      title = \"PAM\")\n\npam &lt;- cluster::pam(dados_norm ,\n                      k = 7)\n\nA fun√ß√£o cluster::pam no R retorna os seguintes elementos:\n\nmedoids: Um vetor contendo os √≠ndices das observa√ß√µes selecionadas como med√≥ides finais de cada cluster.\nclustering: Um vetor contendo os r√≥tulos dos clusters aos quais cada observa√ß√£o foi atribu√≠da.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as dist√¢ncias de isolamento de cada observa√ß√£o em rela√ß√£o ao seu med√≥ide correspondente.\nclusinfo: Uma lista com informa√ß√µes adicionais sobre os clusters, incluindo o n√∫mero de observa√ß√µes em cada cluster e a soma das dist√¢ncias de dissimilaridade intracluster.\n\nEsses elementos fornecem informa√ß√µes sobre os med√≥ides finais selecionados, a atribui√ß√£o de clusters para cada observa√ß√£o, o valor objetivo do agrupamento, as dist√¢ncias de isolamento e informa√ß√µes adicionais sobre cada cluster.\nPara lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a vari√¢ncia total intragrupo para cada agrupamento gerado. A parti√ß√£o que apresentar menor vari√¢ncia total intragrupo √© selecionada como o resultado final do algoritmo. Observe abaixo a aplica√ß√£o para o R:\n\ncotovelo_clara &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::clara ,\n                      method = \"wss\") +\n                  geom_vline( xintercept = 7, linetype = 2) +\n                  labs(x = \"Numero de Grupos\",\n                        y = \"Variancia Total Intragrupo\",\n                        title = \"CLARA\")\nclara &lt;- cluster::clara(dados_norm ,\n                          k = 7, samples = 10)\n\nA fun√ß√£o cluster::clara no R retorna os seguintes resultados:\n\nmedoids: Um objeto pamobject contendo os med√≥ides finais de cada cluster.\nclustering: Um vetor com os r√≥tulos dos clusters atribu√≠dos a cada observa√ß√£o.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as dist√¢ncias de isolamento de cada observa√ß√£o em rela√ß√£o ao seu med√≥ide correspondente.\nclusinfo: Uma lista com informa√ß√µes adicionais sobre os clusters, como o n√∫mero de observa√ß√µes em cada cluster e a soma das dist√¢ncias de dissimilaridade intracluster.\nsamples: Uma lista contendo os √≠ndices das observa√ß√µes selecionadas em cada subamostra.\ncall: A chamada original da fun√ß√£o cluster::clara que foi utilizada.\n\nEsses resultados fornecem detalhes sobre os med√≥ides finais escolhidos, a atribui√ß√£o dos clusters para cada observa√ß√£o, o valor objetivo do agrupamento, as dist√¢ncias de isolamento, informa√ß√µes adicionais sobre os clusters, as subamostras utilizadas e a chamada original da fun√ß√£o.\n\n\n\n9.1.2 M√©todos Hier√°rquicos\nOs m√©todos hier√°rquicos s√£o utilizados para agrupar dados em diferentes n√≠veis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.\nNa abordagem aglomerativa, os grupos s√£o constru√≠dos a partir do n√≠vel mais baixo, onde cada observa√ß√£o forma um cluster separado, at√© atingir o n√≠vel mais alto, onde todos os dados est√£o em um √∫nico grupo. A fus√£o dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, tamb√©m conhecidas como linkages, podem ser definidas da seguinte forma:\n\nM√©todo do vizinho mais pr√≥ximo (Single linkages): Considera a menor dist√¢ncia entre todas as poss√≠veis combina√ß√µes de observa√ß√µes de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\min_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nM√©todo do vizinho mais distante (Complete linkages): Utiliza a maior dist√¢ncia entre todas as poss√≠veis combina√ß√µes de observa√ß√µes de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\max_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nM√©todo da m√©dia das dist√¢ncias (Average linkages): Calcula a m√©dia das dist√¢ncias entre as observa√ß√µes de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\frac{1}{|C_l||C_{l'}|}\\sum_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nM√©todo do centr√≥ide (Centr√≥ide linkages): Considera a dist√¢ncia entre os centr√≥ides de cada grupo como medida de dissimilaridade.\n\n\\[\nd(C_l,C_{l'}) = d^2(c_l,c_{l'}) .\n\\]\n\nM√©todo de Ward: Minimiza a vari√¢ncia dentro dos grupos ao fundir os clusters que levam √† menor varia√ß√£o poss√≠vel.\n\n\\[\nd(C_l,C_{l'}) = \\frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'}) .\n\\]\nNo contexto dos m√©todos hier√°rquicos de agrupamento, a abordagem aglomerativa √© amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada itera√ß√£o √© necess√°rio identificar a melhor divis√£o do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hier√°rquico aglomerativo consiste em:\n\nCada observa√ß√£o √© inicialmente atribu√≠da a um cluster separado.\nCom base no m√©todo de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.\nOs dois grupos com a menor dissimilaridade s√£o fundidos em um √∫nico grupo.\nRepetem-se os passos 2 e 3 at√© que todas as observa√ß√µes estejam em um √∫nico grupo.\n\nJ√° na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo √∫nico que cont√©m todas as observa√ß√µes e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observa√ß√µes. O algoritmo DIANA segue os seguintes passos:\n\nTodas as observa√ß√µes s√£o agrupadas em um √∫nico grupo.\nA observa√ß√£o com a maior dissimilaridade m√©dia em rela√ß√£o aos pontos do mesmo grupo √© separada em um novo grupo.\nCada observa√ß√£o do grupo inicial √© atribu√≠da ao novo grupo se a dissimilaridade m√©dia em rela√ß√£o aos objetos desse grupo for menor do que a dissimilaridade m√©dia em rela√ß√£o aos demais pontos do grupo inicial.\n\n4.Calcula-se o di√¢metro de todos os grupos (a maior dissimilaridade entre duas observa√ß√µes) e seleciona-se o grupo com o maior di√¢metro.\n\nRepetem-se os passos 2 a 4 at√© que cada observa√ß√£o esteja em um grupo separado.\n\nNo agrupamento hier√°rquico, a visualiza√ß√£o dos clusters √© feita por meio de um dendrograma, um gr√°fico ramificado que mostra as jun√ß√µes e divis√µes dos clusters. A altura do ramo no primeiro n√≥ do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o n√∫mero de grupos a partir do dendrograma, busca-se uma grande diferen√ßa de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma caracter√≠stica dos m√©todos hier√°rquicos √© que as decis√µes de agrupamento ou divis√£o n√£o s√£o desfeitas, ou seja, n√£o h√° troca de observa√ß√µes entre os clusters. Decis√µes de uni√£o ou divis√£o mal feitas podem resultar em grupos de baixa qualidade. Al√©m disso, esses m√©todos n√£o s√£o bem dimensionados, pois cada decis√£o de mesclagem ou divis√£o requer a avalia√ß√£o de muitos objetos ou clusters. Pelo exemplo abaixo na Figura¬†9.2, uma poss√≠vel resposta de n√∫mero adequado de clusters seria de dois ou tr√™s grupos.\n\n\n\nFigura¬†9.2: Exemplo de Dendrograma\n\n\nOs agrupamentos hier√°rquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das fun√ß√µes hclust e cluster::diana, √© obtido um objeto da classe ‚Äúhclust‚Äù e da clase ‚Äúdiana‚Äù respectivamente, esses objetos cont√©m informa√ß√µes sobre o agrupamento hier√°rquico realizado, incluindo a estrutura do dendrograma, as dist√¢ncias entre os objetos e outras propriedades relacionadas. Seguido pela sele√ß√£o, com base no dendrograma, do n√∫mero K ideal de clusters (N√£o necessariamente s√≥ um K), e o ‚Äúcorte‚Äù da √°rvore aglomerativa no valor ideal identificado. √â apresentado abaixo para todos os m√©todos citados a aplica√ß√£o para o R, supondo a dist√¢ncia utilizada como a euclidiana calculada no in√≠cio do cap√≠tulo.\n\nlibrary(ggdendro)\n#AGLOMERATIVOS ############\n#METODO WARD -----\n##CRIAR O OBJETO HCLUST\nagl_ward &lt;- hclust(dist_euclidian , method = \"ward.D2\")\n\n# PLOT DO DENDROGRAMA \ndendograma_ward &lt;-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,\n main = \"Ward - cortado em H = 20\")\n\n# \"corte\" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS\nagl_ward_res &lt;- cutree(agl_ward , k = 3:8)\n\n# segue o mesmo para todos os outros metodos\n\n#METODO SINGLE LINKAGE -----\nagl_single &lt;- hclust(dist_euclidian , method = \"single\")\n\ndendograma_single &lt;- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,\n                        main = \"Vizinho mais Proximo - cortado em H = 4\",\n                          xlab = \"\")\n\nagl_single_res &lt;- cutree(agl_single , k = 3:8)\n\n#METODO COMPLETE LINKAGE -----\nagl_complete &lt;-  hclust(dist_euclidian , method = \"complete\")\n\ndendograma_complete &lt;- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,\n                             main = \"Vizinho mais Distante - cortado em H = 10\")\n\nagl_complete_res &lt;- cutree(agl_complete , k = 3:8)\n\n#METODO AVARAGE LINKAGE -----\nagl_ave &lt;- hclust(dist_euclidian , method = \"average\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,\n                                main = \" Media das Distancias - cortado em H = 15\")\n\nagl_ave_res &lt;- cutree(agl_ave, k = 3:8)\n\n#METODO CENTROIDE LINKAGE -----\nagl_cent &lt;- hclust(dist_euclidian , method = \"centroid\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,\n                            main = \" Centroide - cortado em H = 15\")\n\nagl_cent_res &lt;- cutree(agl_cent , k = 3:8)\n\n####### DIVISIVO (DIANA) #########\ndiana &lt;- cluster::diana(dist_euclidian ,\n                            diss = TRUE ,\n                            metric = \"euclidean\",\n                            keep.diss = FALSE ,\n                            keep.data = FALSE\n                            )\n\ndendrograma_diana &lt;-  plot(cut(as.dendrogram(diana), h = 7)$upper ,\n                                   main = \"Diana - cortado em H = 7\")\n\ndiana_res &lt;- cutree(diana , k = 3:8)\n\n\n\n9.1.3 DBSCAN\nO m√©todo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) √© um algoritmo de agrupamento que foi proposto para encontrar clusters com formas arbitr√°rias, ou seja, clusters que n√£o necessariamente possuem uma forma esf√©rica ou convexa. Ele busca identificar as regi√µes mais densas do espa√ßo vetorial separadas por regi√µes com menos objetos, como √© poss√≠vel ver na Figura¬†9.3 sua efici√™ncia em rela√ß√£o a outros m√©todos.\n\n\n\nFigura¬†9.3: Aplica√ß√£o DBSCAN, Fonte: Boyke et al.¬†(2021)\n\n\nA ideia geral do algoritmo DBSCAN √© identificar os clusters de forma que a densidade de pontos ao redor de cada ponto de um grupo seja maior que um limite estabelecido. Ele utiliza dois par√¢metros principais: \\(\\epsilon\\) e MinPts, para entendimento do agrupamento DBSCAN observe a defini√ß√£o dos seguintes termos:\n\n\\(\\epsilon\\)-vizinhos: Os \\(\\epsilon\\)-vizinhos de um ponto \\(x_j\\) s√£o os objetos cuja dist√¢ncia para \\(x_i\\) seja menor ou igual a \\(\\epsilon\\), ou seja, a vizinhan√ßa de \\(x_i\\) √© composta pelos objetos que est√£o dentro de um raio \\(\\epsilon\\) ao redor de \\(x_i\\).\nPonto de N√∫cleo: Um ponto \\(x_i\\) √© considerado um ponto de n√∫cleo se o n√∫mero de seus \\(\\epsilon\\)-vizinhos (ou seja, os pontos dentro da vizinhan√ßa de \\(x_i\\)) for maior ou igual a um valor m√≠nimo estabelecido chamado MinPts.\nPonto de Borda: Um ponto \\(x_i\\) √© considerado um ponto de borda se o n√∫mero de seus \\(\\epsilon\\) -vizinhos for menor do que MinPts, mas ele est√° dentro da vizinhan√ßa de algum ponto de n√∫cleo.\nDiretamente alcan√ß√°vel por densidade: Um ponto \\(x_j\\) √© diretamente alcan√ß√°vel por densidade por um ponto \\(x_i\\) se \\(x_j\\) √© um \\(\\epsilon\\)-vizinho de \\(x_i\\) e se \\(x_i\\) √© um ponto de n√∫cleo.\nAlcan√ß√°vel por densidade: Um ponto \\(x_i\\) √© alcan√ß√°vel por densidade por um ponto \\(x_j\\) se existe uma cadeia de pontos \\(x_{i'}\\) , onde $i‚Äô N \\mathbb{N} $, em que \\(x_1 = x_j, x_N = x_i\\) e \\(x_{i' + 1}\\) √© diretamente alcan√ß√°vel por densidade por \\(x_{i'}\\).\nConectado por densidade: Um ponto \\(x_i\\) √© conectado por densidade a um ponto \\(x_j\\) se existe um terceiro ponto \\(x_{i'}\\) em que ambos s√£o alcan√ß√°veis por densidade por \\(x_{i'}\\).\n\nCom base nesses crit√©rios, o DBSCAN define os grupos da seguinte forma:\n\nSe um ponto \\(x_i\\) √© alcan√ß√°vel por densidade por um ponto \\(x_j\\), ent√£o ambos pertencem ao mesmo cluster \\(C_l\\).\nTodos os pontos de um cluster s√£o conectados por densidade entre si.\n\nUma das vantagens do DBSCAN √© sua capacidade de identificar outliers no conjunto de dados, uma vez que ele n√£o atribui todos os objetos a grupos. Os outliers s√£o definidos como os pontos que n√£o s√£o atribu√≠dos a nenhum cluster. O algoritmo DBSCAN funciona da seguinte maneira:\n\nInicialmente, todas as observa√ß√µes da base de dados s√£o classificadas como ‚Äún√£o visitadas‚Äù.\nEm seguida, os seguintes passos s√£o executados repetidamente:\n(a). Selecionar aleatoriamente uma observa√ß√£o ‚Äún√£o visitada‚Äù \\(x_i\\).\n(b). Verificar o n√∫mero de \\(epsilon\\)-vizinhos da observa√ß√£o \\(x_i\\) selecionada:\n\nSe o n√∫mero de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for maior ou igual a MinPts, um novo cluster √© criado para \\(x_i\\), e todos os objetos na \\(\\epsilon\\)-vizinhan√ßa de \\(x_i\\) s√£o adicionados a um conjunto candidato, \\(M\\). O algoritmo adiciona iterativamente a \\(C_l\\) todos os objetos em ùëÄ que n√£o pertencem a nenhum cluster.\nSe o n√∫mero de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for menor do que MinPts, \\(x_i\\) √© marcado como um ponto de ru√≠do (outlier).\n\nRepetir os passos 1 e 2 at√© que n√£o haja mais observa√ß√µes ‚Äún√£o visitadas‚Äù.\n\nOs par√¢metros \\(\\epsilon\\) e MinPts afetam diretamente o resultado do agrupamento, determinando o tamanho m√≠nimo de cada grupo e a dist√¢ncia entre os clusters. Uma das limita√ß√µes do DBSCAN √© o fato de que esses par√¢metros s√£o globais, ou seja, definidos para todos os grupos formados, o que pode n√£o ser ideal quando diferentes clusters possuem densidades de pontos distintas. Para a sele√ß√£o do valor de MinPts √© comum que seja baseado no problema em si ou conhecimento pr√©vio. Por√©m, uma proposta para sele√ß√£o √© o MinPts = 2√ó\\(p\\) pode ser um bom valor para o par√¢metro. J√° o \\(\\epsilon\\) costuma ser baseado no n√∫mero m√≠nimo de pontos definido, em que √© constru√≠do o gr√°fico das dist√¢ncias de cada ponto para os MinPts‚àí1 vizinhos mais pr√≥ximos e escolhido o valor ‚Äúcotovelo‚Äù, ou seja, onde come√ßa a ocorrer um crescimento mais acentuado nas dist√¢ncias ordenadas. Abaixo vemos como esse m√©todo se d√° de forma aplicada para o software de programa√ß√£o tanto da sele√ß√£o do melhor \\(\\epsilon\\) quanto da aplica√ß√£o do modelo.\n\n## selecao do melhor eps\n dbscan::kNNdist(dados_diss ,\n k = 3, all = FALSE)\n## ajuste com o eps selecionado\n dbscan &lt;- dbscan::dbscan(dados_norm , eps = 2, minPts = 4)\n\nEssa fun√ß√£o retorna os seguintes elementos:\n\ncluster: √â um vetor que atribui um n√∫mero de cluster a cada ponto de dados no conjunto de entrada. Os pontos que n√£o pertencem a nenhum cluster s√£o rotulados como ‚Äú0‚Äù ou como ‚Äúoutlier‚Äù.\neps: √â o valor do raio (epsilon) usado no algoritmo.\nminPts: √â o n√∫mero m√≠nimo de pontos necess√°rios para formar um cluster.\nborder: √â um vetor l√≥gico que indica se cada ponto √© um ponto de borda.\nreachability: √â um vetor que armazena os valores de reachability distance de cada ponto.\ncore_dist: √â um vetor que cont√©m as dist√¢ncias de densidade m√≠nima( core distance ) para cada ponto.\n\nUma t√©cnica relacionada ao DBSCAN que lida com a limita√ß√£o dos par√¢metros globais √© o algoritmo OPTICS (Ordering Points To Identify the Clustering Structure), proposto por (Ankerst et al. 1999). O OPTICS ordena os dados de forma que as observa√ß√µes em grupos mais densos estar√£o mais pr√≥ximas na ordena√ß√£o. Esse algoritmo pode ser uma op√ß√£o para explorar diferentes densidades de pontos em diferentes clusters.\n\n\n9.1.4 Agrupamento Espectral\nAgrupar conjuntos de dados com alta dimensionalidade pode ser desafiador para alguns m√©todos de agrupamento. A grande quantidade de vari√°veis em alta dimens√£o pode aumentar significativamente o custo computacional e dificultar a separa√ß√£o adequada dos grupos. Nesse contexto, os m√©todos de agrupamento espectral surgem como uma abordagem promissora, pois permitem reduzir a dimensionalidade dos dados sem perder a informa√ß√£o contida nas vari√°veis, al√©m de melhorar a separa√ß√£o entre grupos que podem n√£o estar claramente distintos devido √† alta dimensionalidade.\nOs algoritmos de agrupamento espectral usam uma medida de dissimilaridade para representar o conjunto de dados como um grafo. Nessa representa√ß√£o, os v√©rtices do grafo correspondem √†s observa√ß√µes (\\(V = v_1, ..., v_N\\)), e as arestas (\\(A\\)) s√£o definidas por uma matriz de similaridade, onde \\(a(x_i, x_{i'})\\) representa o peso da aresta que conecta os v√©rtices \\(x_i\\) e \\(x_{i'}\\). O agrupamento em \\(K\\) clusters √© ent√£o formulado como um problema de corte de arestas, que pode ser computacionalmente complexo. Para contornar essa complexidade, √© aplicada a teoria espectral.\nO algoritmo de Ng, Jordan e Weiss (NJW) √© um exemplo de algoritmo de agrupamento espectral. Ele requer a defini√ß√£o pr√©via de um par√¢metro, assim como o algoritmo K-m√©dias. Esse par√¢metro √© o n√∫mero de grupos \\(K\\) a serem formados. O algoritmo NJW segue os seguintes passos:\n\nCalcula-se a matriz de similaridade \\(A\\), usando um par√¢metro escalar \\(\\sigma\\). A similaridade entre cada par de pontos \\(x_i\\) e \\(x_{i'}\\) √© medida usando uma fun√ß√£o de similaridade, como a fun√ß√£o Gaussiana. A matriz \\(A\\) √© preenchida com os valores calculados.\nCalcula-se a matriz de graus \\(D\\), onde cada elemento \\(d_{ij}\\) na diagonal principal representa a soma das similaridades da linha \\(i\\) da matriz \\(A\\). A matriz \\(D\\) captura a estrutura de conectividade do conjunto de dados.\nConstr√≥i-se a matriz Laplaciana \\(L\\), que √© uma forma de normaliza√ß√£o de \\(A\\). No algoritmo NJW, a matriz Laplaciana √© definida como \\(L = D^{(-1/2)} ùê¥ ùê∑^{(-1/2)}\\). Essa normaliza√ß√£o real√ßa as diferen√ßas entre os grupos de dados.\nIdentificam-se os \\(K\\) maiores autovalores de \\(L\\) e seus respectivos autovetores associados, que s√£o armazenados em uma matriz \\(Z\\) de tamanho \\(N \\times K\\).\nDefine-se uma nova matriz \\(Y\\), normalizada a partir de \\(Z\\). Cada elemento \\(y_{ii'}\\) de \\(Y\\) √© calculado dividindo-se o elemento \\(z_{ii'}\\) de \\(Z\\) pelo produto da raiz quadrada do elemento diagonal correspondente a \\(z_{ii}\\) e a raiz quadrada do elemento diagonal correspondente a \\(z_{i'}\\). Essa normaliza√ß√£o ajusta as magnitudes dos dados.\nCom a redu√ß√£o da dimensionalidade dos dados na matriz \\(Y\\) (de tamanho \\(N \\times K\\)), pode-se aplicar um algoritmo de agrupamento, como o K-m√©dias, para realizar o agrupamento dos dados.\n\n\nspectral_clustering &lt;- function(data , # dados\n                                k) # numero de grupos\n                                {\n    # matriz de dissimilaridade\n    A &lt;- as.matrix(KRLS::gausskernel (data , 2)) # sigma^2 = 1\n    diag(A) &lt;- 0\n    \n    # matriz Laplaciana\n    L = diag(1 / sqrt(rowSums(A))) %*% A %*% diag(1 / sqrt(rowSums(A)))\n    \n    # matriz de autovetores associados aos k maiores autovalores\n    auto &lt;- eigen(L, symmetric = TRUE)\n    X &lt;- auto$vectors[, 1:k]\n    \n    # matriz normalizada\n    Y &lt;- X / sqrt(rowSums(X ^ 2))\n    \n    # agrupamente k medias na matriz redimensionalizada\n    set.seed (1122)\n    k_means &lt;- kmeans(Y, centers = k)\n    \n    return(list(\n    clusters = k_means$cluster , #retornar grupos\n    auto = auto , #retornar autovalores e autovetores\n    autovetores = X, #retornar autovetores dos K maiores autovalores\n    normalizada = Y #retornar matriz normalizada\n    ))\n}\n\nA escolha dos par√¢metros \\(K\\) e \\(\\sigma\\) pode ser feita de forma arbitr√°ria, dependendo do objetivo do estudo. No entanto, existem abordagens e t√©cnicas dispon√≠veis para auxiliar na sele√ß√£o desses par√¢metros. No exemplo acima , o par√¢metro \\(\\sigma\\) foi fixado em 1, e o n√∫mero de grupos \\(K\\) foi escolhido com base na an√°lise dos primeiros autovalores da matriz Laplaciana \\(L\\). O valor √≥timo de \\(K\\) √© aquele em que ocorre uma queda significativa dos autovalores, uma vez que os maiores autovalores da matriz Laplaciana tendem a ser pr√≥ximos de 1.\n\n\n9.1.5 Valida√ß√£o do Modelo\nNo problema do agrupamento, n√£o √© poss√≠vel verificar o grau de acerto do resultado obtido, uma vez que os verdadeiros grupos n√£o s√£o conhecidos a priori. Portanto, √© necess√°rio aplicar algum tipo de valida√ß√£o √† parti√ß√£o final.\nQuatro √≠ndices de avalia√ß√£o interna da qualidade do agrupamento s√£o os principais a serem utilizados atualmente: Davies-Bouldin (DB), Dunn (D), Silhueta (S) e Calinski-Harabasz (CH).\n\n9.1.5.1 Davies-Bouldin (DB):\nO √≠ndice DB mede a similaridade m√©dia entre cada grupo e seu grupo mais similar dentre os demais clusters. A dist√¢ncia m√©dia \\(\\delta_l\\) de um grupo \\(l\\) √†s suas observa√ß√µes √© calculada em rela√ß√£o a um valor referencial \\(m_l\\). A f√≥rmula para \\(\\delta_l\\) √©:\n\\[\n\\delta_{l} = \\left(\\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} \\left \\| x_{i} - m_{l} \\right \\|^q\\right)^{\\frac{1}{q}}.\n\\]\nonde \\(n_l\\) √© o n√∫mero de observa√ß√µes no grupo \\(l\\), \\(x_i\\) √© uma observa√ß√£o, \\(m_l\\) √© o valor referencial (centr√≥ide ou med√≥ide) e \\(q\\) √© um valor pr√©-definido, onde os valores mais utilizados s√£o \\(q = 1\\) e \\(q= 2\\). Para o √≠ndice, √© utilizada tamb√©m dist√¢ncia entre os grupos \\(\\Delta_{ll'}\\), que √© obtida como a dist√¢ncia entre os valores referenciais de cada grupo. A f√≥rmula para \\(\\Delta_{ll'}\\) √©:\n\\[\n\\Delta_{ll'} = \\left( \\sum_{j=1}^{p} |m_{jl} - m_{jl'}|^{t} \\right)^{\\frac{1}{t}}.\n\\]\nonde \\(t \\in \\mathbb{N}\\) √© pr√©-definido e geralmente adotado como \\(t = 1\\) (dist√¢ncias absolutas) ou \\(t=2\\) (dist√¢ncia euclidiana). Assim, o √≠ndice de Davies-Bouldin (DB) fica definido por:\n\\[\nDB = \\frac{1}{K} \\sum_{l=1}^{K} \\max_{l \\neq l'} \\left( \\frac{\\delta_{l} + \\delta_{l'}}{\\Delta_{ll'}} \\right).\n\\] Nesse caso, buscamos agrupar observa√ß√µes de forma a minimizar a vari√¢ncia intragrupo e maximizar a diferen√ßa entre eles. Portanto, valores menores do √≠ndice de Davies-Bouldin s√£o considerados melhores. Observe a m√©trica aplicada ao conjunto de dados de agrupamento obtido pelo k-m√©dias (As medidas seguintes tamb√©m ser√£o aplicadas ao mesmo conjunto).\n\n#Metrica DB usando Centroide\nclusterSim::index.DB(dados_norm ,\n                     k_medias$cluster)$DB\n#Metrica DB usando Medoide\nclusterSim::index.DB(dados_norm ,\n                      k_medias$cluster ,\n                      d = dist_euclidian ,\n                      centrotypes = \"medoids\"\n                      )$DB \n\n\n\n9.1.5.2 Dunn (D):\nO √≠ndice D mede a raz√£o entre a separa√ß√£o dos grupos e a vari√¢ncia dentro deles. A separa√ß√£o entre dois grupos \\(l\\) e \\(l'\\) √© calculada pela dist√¢ncia do vizinho mais pr√≥ximo, dessa forma buscamos valores grandes para essa medida de avalia√ß√£o. A vari√¢ncia intragrupo √© representada pelo di√¢metro \\(diam_l\\) do cluster. A f√≥rmula para o √≠ndice D √©:\n\\[\nD = \\frac{{\\min_{l,l' \\in \\{1,...,K\\}; l \\neq l'} d(C_{l}, C_{l'})}}{{\\max_{l \\in \\{1,...,K\\}} diam_{l}}}.\n\\]\n\n#Metrica D\n clValid::dunn(distance = dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.3 Silhueta (S):\nO √≠ndice de Silhueta(S) mede a qualidade do agrupamento considerando as dist√¢ncias de cada ponto em rela√ß√£o √†s observa√ß√µes do mesmo grupo e aos demais clusters formados. Para cada observa√ß√£o \\(i\\) pertencente ao grupo \\(C_l\\), definimos as medidas \\(a_i\\) e \\(b_i\\) da seguinte maneira:\n$$\n\\[\\begin{split}\na_i = \\frac{1}{n_{l}-1} \\sum_{i' \\neq i, i' \\in C_{l}}^{n_l} d(x_{i}, x_{i'}),\\\\\n\nb_i = \\min_{l \\neq l'} \\left( \\frac{1}{n_{l'}} \\sum_{i' \\in C_{l'}}^{n_{l'}} d(x_{i}, x_{i'}) \\right).\n\\end{split}\\]\n$$\nonde \\(d(x_i,x_{i'})\\) representa a dist√¢ncia entre as observa√ß√µes \\(x_i\\) e \\(x_{i'}\\). A partir dessas medidas, calculamos a silhueta \\(s_i\\) da observa√ß√£o \\(i\\), com \\(i = 1, ..., N\\), usando a f√≥rmula:\n\\[\ns_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}.\n\\]\nO coeficiente de Silhueta S global √© obtido por:\n\\[\nS = \\frac{1}{K} \\sum_{l=1}^{K} \\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} s_i.\n\\] Dessa forma, valores maiores de S indicam agrupamentos mais densos e separados, o que √© considerado um cen√°rio adequado para um agrupamento bem-sucedido.\n\n#Metrica S\nclusterSim::index.S(dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.4 Calinski-Harabasz (CH):\nO √≠ndice de Calinski-Harabasz (CH) considera a vari√¢ncia intra-grupo, chamada de \\(WGGS_l\\), de cada cluster \\(C_l\\) gerado, levando em conta a dist√¢ncia quadr√°tica de cada observa√ß√£o em rela√ß√£o ao seu valor de refer√™ncia \\(m_l\\), que pode ser um centr√≥ide ou med√≥ide. A vari√¢ncia intra-grupo total,\\(WGGS\\), √© calculada como a soma das vari√¢ncias intra-grupo de cada cluster:\n\\[\nWGGS = \\sum_{l=1}^{K} \\sum_{i=1}^{n_{l}} d^2(x_{i}, m_l).\n\\]\nAl√©m disso, o c√°lculo do √≠ndice CH utiliza uma medida de dispers√£o \\(BGSS\\) entre os grupos, que √© obtida atrav√©s da soma ponderada das dist√¢ncias quadr√°ticas do valor de refer√™ncia de cada cluster em rela√ß√£o a um valor central global \\(m\\). Essa medida √© calculada da seguinte forma:\n\\[\nBGSS = \\sum_{l=1}^{K} n_{l} d^2(m_l, m).\n\\]\nDessa forma, o √≠ndice CH de Calinski-Harabasz √© dado por:\n\\[\nCH = \\frac{(N - K) }{K - 1}\\times \\frac{BGSS}{WGSS}.\n\\]\nOnde, valores maiores do √≠ndice CH indicam grupos com menor vari√¢ncia e bem separados, o que √© considerado uma boa caracter√≠stica de um agrupamento.\n\n#Metrica CH usando Medoide\nclusterSim::index.G1(dados_norm , k_medias$cluster , \n                     d = dist_euclidian , centrotypes = \"medoids\")\n\n#Metrica CH usando centroide\nclusterSim::index.G1(dados_norm , k_medias$cluster)\n\n\n\n\n\nAnkerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, e J√∂rg Sander. 1999. ¬´OPTICS: Ordering points to identify the clustering structure¬ª. ACM Sigmod record 28 (2): 49‚Äì60.\n\n\nHartigan, John A, e Manchek A Wong. 1979. ¬´Algorithm AS 136: A k-means clustering algorithm¬ª. Journal of the royal statistical society. series c (applied statistics) 28 (1): 100‚Äì108.\n\n\nKaufman, Leonard, e Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. ¬´Least squares quantization in PCM¬ª. IEEE transactions on information theory 28 (2): 129‚Äì37."
  },
  {
    "objectID": "PCA.html#introdu√ß√£o.",
    "href": "PCA.html#introdu√ß√£o.",
    "title": "10¬† An√°lise de Componentes Principais (PCA).",
    "section": "10.1 Introdu√ß√£o.",
    "text": "10.1 Introdu√ß√£o.\nA An√°lise de Componentes Principais (PCA) √© uma t√©cnica que busca resumir a varia√ß√£o presente em um conjunto de dados multivariados atrav√©s de combina√ß√µes lineares de suas vari√°veis originais, que s√£o correlacionadas. O objetivo principal √© reduzir a dimensionalidade dos dados, representando um grande n√∫mero de vari√°veis originais em um n√∫mero menor de componentes principais. Essas novas vari√°veis s√£o ordenadas em ordem decrescente de import√¢ncia, de modo que a primeira componente principal capture a maior quantidade poss√≠vel da variabilidade total dos dados, e as subsequentes capturem cada vez menos.\nA primeira componente principal √© calculada de forma a maximizar a vari√¢ncia explicada pelos dados, ou seja, ela √© a dire√ß√£o ao longo da qual os dados apresentam a maior varia√ß√£o poss√≠vel. As demais componentes s√£o ortogonais √† primeira e s√£o combina√ß√µes lineares das vari√°veis originais, indicando a import√¢ncia relativa de cada vari√°vel naquele componente. Note ent√£o, que por essa defini√ß√£o j√° podemos perceber a rela√ß√£o com os autovetores e autovetores associados.\nO PCA tem diversas aplica√ß√µes em √°reas como estat√≠stica, engenharia e ci√™ncia de dados, sendo usado para resumir grandes conjuntos de dados, detectar padr√µes e estruturas latentes, identificar outliers e reduzir o ru√≠do presente nos dados. Al√©m disso, ele √© comumente utilizado como uma t√©cnica de pr√©-processamento de dados para outras t√©cnicas de an√°lise, como regress√£o e clustering.\nUma aplica√ß√£o comum do PCA √© na identifica√ß√£o de vari√°veis latentes em um conjunto de dados, que n√£o s√£o diretamente observ√°veis, mas podem ser inferidas a partir de outras vari√°veis observ√°veis correlacionadas entre si.Um exemplo de vari√°vel latente na obstetr√≠cia pode ser a ‚Äúsa√∫de fetal‚Äù ou at√© mesmo a ‚Äúsa√∫de materna‚Äù. N√£o pode ser diretamente medida ou observada, mas pode ser inferida a partir de m√∫ltiplas vari√°veis observ√°veis, como a frequ√™ncia card√≠aca fetal, a press√£o arterial materna entre outras."
  },
  {
    "objectID": "PCA.html#como-realizar-a-an√°lise-de-componentes-principais.",
    "href": "PCA.html#como-realizar-a-an√°lise-de-componentes-principais.",
    "title": "10¬† An√°lise de Componentes Principais (PCA).",
    "section": "10.2 Como realizar a an√°lise de componentes principais.",
    "text": "10.2 Como realizar a an√°lise de componentes principais.\nO primeiro passo √© entender a defini√ß√£o matem√°tica real das componentes principais. Seja \\(\\boldsymbol{X}\\) um vetor aleat√≥rio com \\(\\boldsymbol{\\mu} = E(\\boldsymbol{X})\\) e \\(\\boldsymbol \\Sigma = Var(\\boldsymbol{X})\\) e \\((\\lambda_i,e_i), i = 1,\\dots,p\\) os pares de autovalores e autovetores normalizados associados de \\(\\Sigma\\). Ent√£o,\n\\[\n\\begin{split}\n\\boldsymbol{Y} = \\boldsymbol{ O'}\\boldsymbol{X},\\quad \\textrm{com}\\quad \\boldsymbol{O} = [\\boldsymbol{e}_1,\\boldsymbol{e}_2,\\dots,\\boldsymbol{e}_p],\\textrm{ os componentes principais de }\\boldsymbol X\\\\\n\\textrm{ou seja}\\\\\n\\boldsymbol Y =\n\\begin{bmatrix}\nY_1\\\\\n\\vdots\\\\\nY_p\n\\end{bmatrix} \\textrm{ com  } \\quad Y_1 = \\boldsymbol e_1'\\boldsymbol X = e_{11}X_1 + e_{12}X_2 +  \\dots + e_{1p}X_p,\n\\end{split}\n\\]\na primeira componente principal.\nOs elementos na matriz \\(\\boldsymbol{O}\\), denominados loadings, representam as rela√ß√µes entre as vari√°veis originais e as componentes principais. Cada coluna da matriz \\(\\boldsymbol{O}\\) corresponde a uma componente principal, e as linhas correspondem √†s vari√°veis originais. Os elementos \\(e_{ij}\\) s√£o os coeficientes de pondera√ß√£o que indicam a contribui√ß√£o de cada vari√°vel para cada componente principal. Em outras palavras, eles representam a import√¢ncia relativa das vari√°veis na constru√ß√£o das componentes principais.\nComo citado, as componentes ent√£o seguem sendo combina√ß√µes lineares das vari√°veis originais e os autovetores correspondentes. Os componentes principais de \\(\\boldsymbol{X}\\), \\(\\boldsymbol{Y}\\), s√£o tais que,\n\\[\n\\begin{split}\n\\boldsymbol\\mu_y = E(\\boldsymbol Y) = E(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'E(\\boldsymbol X) = \\boldsymbol O'\\boldsymbol\\mu_x\\\\\n\\boldsymbol\\Sigma_y = Var(\\boldsymbol Y) = Var(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'Var(\\boldsymbol X)\\boldsymbol O = \\boldsymbol{O'\\Sigma_xO= \\Lambda}.\n\\end{split}\n\\]\nOu seja, as componentes principais s√£o constru√≠das de forma que elas sejam n√£o correlacionadas entre si (\\(cov(Y_i,Y_j) = 0\\), para todo \\(i \\neq j\\)) e tenham vari√¢ncias iguais aos autovalores correspondentes (\\(Var(Y_i) = \\lambda_i\\)). A prova desse resultado pode ser vista em (Johnson, Wichern, et al. 2002, 5:432).\n√â de conhecimento geral, como um dos objetivos da an√°lise de dados, a compreens√£o da distribui√ß√£o bem como variabilidade dos dados. Podemos ent√£o descrever a vari√¢ncia total da popula√ß√£o como sendo o somat√≥rio de todos os autovalores \\(\\lambda_i\\). A partir disso, podemos escrever a propor√ß√£o da vari√¢ncia total explicada pela \\(j\\)-√©sima componente, como sendo:\n\\[\n\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i}; \\qquad \\forall j =1,\\dots,p.\n\\] Essa defini√ß√£o √© muito √∫til para reduzir a dimensionalidade dos dados, pois nos permite capturar uma propor√ß√£o significativa da variabilidade total com um n√∫mero menor de componentes. Dessa forma, para algum \\(p\\) significativamente grande, podemos utilizar \\(d &lt; p\\) componentes ao inv√©s das \\(p\\) vari√°veis originais, considerando que, podemos descrever uma propor√ß√£o relativamente alta da vari√¢ncia com essas \\(d\\) componentes.\nSe \\(Y_i = \\boldsymbol{e'_iX}, i =1\\dots,p\\) s√£o as componentes principais obtidas da matriz de covari√¢ncia, ent√£o\n\\[\n\\rho_{Y_i,X_j} = \\frac{e_{ij}\\sqrt{\\lambda_i}}{\\sigma_{jj}}, \\quad \\forall i,j=1,\\dots p,\n\\]\nS√£o os coeficientes de correla√ß√£o entre a componente \\(Y_i\\) e a vari√°vel \\(X_j\\). Esse valor auxiliar√° na compreens√£o entre a rela√ß√£o indiv√≠dual de \\(X_j\\) a uma componente principal \\(Y_i\\), por√©m n√£o explica a rela√ß√£o desta vari√°vel em presen√ßa das outras. Por isso, alguns altores recomendam o uso √∫nico do valor de \\(e_{ij}\\) para compreender a rela√ß√£o vari√°vel-componente. Ambos os resultados ser√£o import√¢ntes para compreens√£o da componente. Observe abaixo a aplica√ß√£o de todos os conceitos discutidos no software utilizado ( R ), com os devidos coment√°rios.\n\n#Importacao dos dados\ndados &lt;- readRDS('dados/dados_indicadores.rds')\n\n#Matriz de Covariancia\nmatriz_cov &lt;- cov(dados |&gt; dplyr::select_if(is.numeric))\n\n#Calcular os autovalores e autovetores\nresultado &lt;- eigen(matriz_cov)\nautovalores &lt;- resultado$values\nautovetores &lt;- resultado$vectors\n\n#Calcular as componentes principais\ncomponentes_principais &lt;- as.matrix(dados |&gt; dplyr::select_if(is.numeric)) %*% autovetores\n\n#Calcular as proporceos da variancia total explicadas por cada componente\nprop_variancia &lt;- autovalores / sum(autovalores)\n\n#Calcular os coeficientes de correlacao entre as variaveis originais e as componentes\ncoef_correlacao &lt;- autovetores * sqrt(autovalores) / matriz_cov\n\n##### OU DE FORMA ANALOGA TAMBEM PODEMOS FAZER ###########\npca_dados &lt;- princomp((dados |&gt; dplyr::select_if(is.numeric)))\n\nOnde a fun√ß√£o princomp √© uma fun√ß√£o em R que realiza a an√°lise de componentes principais. Essa fun√ß√£o retorna um objeto do tipo ‚Äúprincomp‚Äù, que cont√©m v√°rias informa√ß√µes sobre a an√°lise realizada. Alguns dos principais elementos retornados por essa fun√ß√£o s√£o:\n\nsdev: Vetor contendo os desvios padr√£o estimados das componentes principais.\nloadings: Matriz de carga, que cont√©m os coeficientes de pondera√ß√£o das vari√°veis originais em cada componente principal. Cada coluna representa um componente principal, e as linhas correspondem √†s vari√°veis originais.\ncenter: Vetor contendo as m√©dias das vari√°veis originais usadas na an√°lise.\nscale: Vetor contendo os desvios padr√£o das vari√°veis originais usadas na an√°lise.\nn.obs: N√∫mero de observa√ß√µes usadas na an√°lise.\nscores: Matriz de escores, que cont√©m os valores das observa√ß√µes nas componentes principais. Cada coluna representa um componente principal, e as linhas correspondem √†s observa√ß√µes.\ncall: A chamada da fun√ß√£o princomp.\n\nAs componentes principais podem ser obtidas a partir de vari√°veis padronizadas. A padroniza√ß√£o das vari√°veis √© comumente realizada na an√°lise de componentes principais para garantir que as vari√°veis tenham a mesma escala e evitar que uma vari√°vel com maior variabilidade domine a an√°lise em detrimento das outras. Bem como estudado em vari√°veis aleat√≥rias, a padroniza√ß√£o das vari√°veis √© feita subtraindo-se a m√©dia de cada vari√°vel e dividindo-se pelo desvio padr√£o. Dessa forma, todas as vari√°veis ter√£o m√©dia zero e desvio padr√£o igual a um.\nNo c√≥digo anterior, a padroniza√ß√£o das vari√°veis n√£o foi realizada, porem √© recomendado padronizar as vari√°veis antes de realizar qualquer an√°lise de componentes principais, isso pode ser feito de maneira simples utilizando o comando scale. Ao realizar essa transforma√ß√£o, trabalhar com a matriz de covari√¢ncias das vari√°veis transformadas equivale a trabalhar com a matriz de correla√ß√£o."
  },
  {
    "objectID": "PCA.html#n√∫mero-de-componentes-principais-e-redu√ß√£o-da-dimensionalidade.",
    "href": "PCA.html#n√∫mero-de-componentes-principais-e-redu√ß√£o-da-dimensionalidade.",
    "title": "10¬† An√°lise de Componentes Principais (PCA).",
    "section": "10.3 N√∫mero de componentes principais e Redu√ß√£O da dimensionalidade.",
    "text": "10.3 N√∫mero de componentes principais e Redu√ß√£O da dimensionalidade.\nA escolha do n√∫mero ideal de componentes principais √© uma etapa crucial ao aplicar a An√°lise de Componentes Principais (PCA) em um conjunto de dados. A sele√ß√£o adequada de componentes pode garantir a reten√ß√£o da maior parte da vari√¢ncia dos dados, ao mesmo tempo em que reduz a dimensionalidade do problema. Abaixo s√£o apresentados alguns dos m√©todos atualmente utilizados para n√∫mero \\(d\\) de componentes retidas.\n\nVari√¢ncia explicada\n\nPlotar a porcentagem de vari√¢ncia explicada de cada componente individual e a porcentagem de vari√¢ncia total capturada por todos os componentes principais. Este √© o m√©todo mais avan√ßado e eficaz que pode ser usado para selecionar o melhor n√∫mero de componentes principais para o conjunto de dados. Neste m√©todo, criamos o seguinte tipo de gr√°fico apresentado na Figura¬†10.1.\n\n#Padronizacao dos dados e criacao do modelo PCA\npca_dados &lt;- princomp(scale(dados |&gt; dplyr::select_if(is.numeric)))\n\n#Scree plot para numero de variaveis\nfactoextra::fviz_eig(pca_dados,\n                     addlabels = TRUE, \n                     linecolor = \"Red\", ylim = c(0, 50)) +\n ggplot2::labs(x = \"Dimens√µes\", y = \"Porcentagem de Vari√¢ncia Explicada\")\n\n\n\n\nFigura¬†10.1: Screeplot para sele√ß√£o de n√∫mero de clusters\n\n\nO n√∫mero de barras √© igual ao n√∫mero de vari√°veis no conjunto de dados original. Neste gr√°fico, cada barra mostra a porcentagem de vari√¢ncia explicada de cada componente individual. Ao observar esse gr√°fico, podemos decidir quantas componentes devem ser mantidas com base em algum crit√©rio como porcentagem m√≠nima que deseja-se manter, exemplo 80% ou 90%, mantendo por exemplo 8 ou 7 componentes. Ou ainda podemos nos basear na linha vermelha usando o m√©todo do cotovelo aplicado em aprendizado n√£o supervisionado.\n\nSeguir a regra de Kaiser\n\nDe acordo com a regra de Kaiser, √© recomendado manter todos os componentes com autovalores maiores que 1.\nVoc√™ pode obter os autovalores da seguinte forma:\n\npca_dados$sdev ^2\n\nEm seguida, voc√™ pode selecionar os componentes com autovalores maiores que 1. Ao seguir essa regra, √© melhor combinar isso com o gr√°fico de porcentagem de vari√¢ncia explicada discutido no M√©todo anterior. √Äs vezes, um autovalor ligeiramente menor que 1 (por exemplo, 0.95) pode capturar uma quantidade significativa de vari√¢ncia nos dados. Portanto, √© melhor mant√™-lo tamb√©m se ele mostrar uma barra relativamente alta no gr√°fico de porcentagem de vari√¢ncia explicada.\n\nM√©trica de avalia√ß√£o de desempenho\n\nEste m√©todo s√≥ pode ser usado se voc√™ planeja realizar tarefas de regress√£o ou classifica√ß√£o com o conjunto de dados reduzido (transformado) ap√≥s aplicar o PCA.\nUsando o gr√°fico discutido no M√©todo de vari√¢ncia explicada, voc√™ pode selecionar o n√∫mero inicial de componentes principais e obter o conjunto de dados reduzido (transformado). Em seguida, voc√™ constr√≥i um modelo de regress√£o ou classifica√ß√£o e mede seu desempenho por meio do RMSE ou da pontua√ß√£o de precis√£o. Em seguida, voc√™ altera ligeiramente o n√∫mero de componentes principais, constr√≥i o modelo novamente e mede a pontua√ß√£o de desempenho. Ap√≥s repetir essas etapas v√°rias vezes, voc√™ pode selecionar o melhor n√∫mero de componentes principais que realmente oferece a melhor pontua√ß√£o de desempenho.\nObserve que a pontua√ß√£o de desempenho do modelo depende de outros fatores tamb√©m. Por exemplo, depende do estado aleat√≥rio da divis√£o dos conjuntos de treinamento e teste, quantidade de dados, desequil√≠brio de classes, n√∫mero de √°rvores no modelo (se o modelo for uma floresta aleat√≥ria ou uma variante similar), n√∫mero de itera√ß√µes definidas durante a otimiza√ß√£o, valores discrepantes e valores ausentes nos dados, etc. Portanto, tenha cuidado ao usar este m√©todo!"
  },
  {
    "objectID": "PCA.html#interpreta√ß√£o-do-componentes-principais-de-uma-amostra.",
    "href": "PCA.html#interpreta√ß√£o-do-componentes-principais-de-uma-amostra.",
    "title": "10¬† An√°lise de Componentes Principais (PCA).",
    "section": "10.4 Interpreta√ß√£o do componentes principais de uma amostra.",
    "text": "10.4 Interpreta√ß√£o do componentes principais de uma amostra.\nAp√≥s realizar a An√°lise de Componentes Principais (PCA) em um conjunto de dados, √© importante interpretar os componentes principais resultantes para entender a estrutura e os padr√µes presentes nos dados. A interpreta√ß√£o dos componentes principais envolve analisar os coeficientes de pondera√ß√£o das vari√°veis originais em cada componente, ou loadings, como apresentado nas sess√µes anteriores.\nUma forma comum de interpretar os loadings √© observar os valores absolutos e as diferen√ßas entre eles. Valores absolutos altos indicam uma forte contribui√ß√£o da vari√°vel para a respectiva componente principal, enquanto valores baixos indicam uma contribui√ß√£o fraca. Al√©m disso, diferen√ßas significativas entre os valores absolutos dos loadings de uma vari√°vel em diferentes componentes podem indicar a presen√ßa de padr√µes espec√≠ficos relacionados a essa vari√°vel.\nPor exemplo, suponha que estamos analisando um conjunto de dados com indicadores de sa√∫de que inclui vari√°veis como taxa de mortalidade infantil, taxa de natalidade, gastos com sa√∫de per capita, entre outras. Ap√≥s realizar a PCA, obtemos as componentes principais. Podemos ent√£o examinar os loadings para interpretar os padr√µes presentes nos dados.\nSuponha que a primeira componente principal tenha altos loadings positivos para as vari√°veis de taxa de natalidade e gastos com sa√∫de per capita, enquanto tem um loading negativo para a taxa de mortalidade infantil. Isso indica que a primeira componente principal est√° capturando um padr√£o em que pa√≠ses com maiores taxas de natalidade e maiores gastos com sa√∫de per capita tendem a ter menores taxas de mortalidade infantil.\nDa mesma forma, a segunda componente principal pode ter altos loadings positivos para as vari√°veis de incid√™ncia de doen√ßas e taxa de vacina√ß√£o, indicando que ela est√° capturando um padr√£o em que pa√≠ses com maiores taxas de incid√™ncia de doen√ßas e maiores taxas de vacina√ß√£o tendem a ter menores valores nessa componente principal.\nA interpreta√ß√£o dos componentes principais tamb√©m pode ser facilitada ao observar os valores dos coeficientes de correla√ß√£o entre as vari√°veis originais e as componentes principais. Esses coeficientes de correla√ß√£o indicam a for√ßa e a dire√ß√£o da rela√ß√£o entre cada vari√°vel e cada componente. Valores absolutos altos indicam uma correla√ß√£o forte, enquanto valores pr√≥ximos a zero indicam uma correla√ß√£o fraca.\n\n\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Refer√™ncias",
    "section": "",
    "text": "Ankerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, and J√∂rg Sander.\n1999. ‚ÄúOPTICS: Ordering Points to Identify the Clustering\nStructure.‚Äù ACM Sigmod Record 28 (2): 49‚Äì60.\n\n\nAnton, Howard, and Chris Rorres. 2001. √Ålgebra Linear\nCom Aplica√ß√µes. Vol. 8. Bookman Porto\nAlegre.\n\n\nHartigan, John A, and Manchek A Wong. 1979. ‚ÄúAlgorithm AS 136: A\nk-Means Clustering Algorithm.‚Äù Journal of the Royal\nStatistical Society. Series c (Applied Statistics) 28 (1): 100‚Äì108.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied\nMultivariate Statistical Analysis. Vol. 5. 8. Prentice hall Upper\nSaddle River, NJ.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. ‚ÄúLeast Squares Quantization in PCM.‚Äù\nIEEE Transactions on Information Theory 28 (2): 129‚Äì37."
  }
]